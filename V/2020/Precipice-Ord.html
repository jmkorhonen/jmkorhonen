<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Precipice-Ord</title>
	<meta name='Generator' content='Zim 0.73.5'>
	<style type='text/css'>
		a          { text-decoration: none      }
		a:hover    { text-decoration: underline }
		a:active   { text-decoration: underline }
		strike     { color: grey                }
		u          { text-decoration: none;
					 background-color: yellow   }
		tt         { color: #2e3436;            }
		pre        { color: #2e3436;
					 margin-left: 20px          }
		h1         { text-decoration: underline;
					 color: #4e9a06; margin-bottom: 0 }
		h2         { color: #4e9a06; margin-bottom: 0 }
		h3         { color: #4e9a06; margin-bottom: 0 }
		h4         { color: #4e9a06; margin-bottom: 0 }
		h5         { color: #4e9a06; margin-bottom: 0 }
		p          { margin-top: 0              }
		span.zim-tag {
			color: #ce5c00;
		}
		div.zim-object {
			border-style:solid;
			border-width:1px;
		}
		.checked-box {list-style-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAANOgAADMQBiN+4gQAAAAd0SU1FB9gKGQ8sMEGsKGkAAAAZdEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVBXgQ4XAAAEBUlEQVRIx62V22tdRRTGf7Nn73P2ybntnNOe3NqkPTGgLTVUUZF6QatSLOKTPgqCIqLgQ0H/A1sQQbBYCBb1QfAxiC8tSO1FqHkwJVKtjdTGNraUmObsc9nXmfGh7cGYpM1D5nHWzPetteZb3wg2eB2YqYm4zSadsMtoboiNBH/3TE0awx6j+MRoxoTg/IYRvP19TQrJS0bzhdHGSyKFkLTtjSKwMjyiEz43ynhtP6bdjBCWyFobAf7eT7VhNF/q1FRbjYjmUohlCVPwnB+6FUxMTJipqSmUUhhjEGKd3bMT4ks/Y6oLBK2Yth8hHYtCJXOix7Nf7xLMzc0xOzvLzp078TyPNE3viW3QJPXzhNWbxFFKHCmMhoLn/FHodd48vGfhapdAacXQlkFK5dL6wIUm6fuTZPuvqDQhaMUYYyiVyuQr6rXDexYuAdi3tSv1ZJNs/R/CaszzT+1na88uXFnCEnJVgivBNN8uTJKmHQI/ptOOcXNZzMz9mOqFs90OHpipWcYwlo5P4ebnuOkrvr5wgrH+h3im7y36MzuwRXYZeKha/OhP0EkadFoxQSdGSotedR/+XwMc2XvKdNUFOFqZx6LKZWIiwjgkikNmLp/hm8sH+K1zjFTHXfBYdTi+eJArzXM0GxFxoBDCopLvo/fqEwi1XPkWkGqjFo2TgB1jOYZUKZTS/D1/ncmLh7jon0IbRWoiTi59ymzzJEEQE3cStNZsGxqlfPE57MBbOR8fP3hDGalOO9fq2DlBvmZw8xa2IxACGn6TydlD/O6f5OzSV/zif0cYhLQaEXGkKBbz7Ov/AOlXV1cxgBJRI3fuSTrpTawt18kWIZN1CFuaONI0w0WOXfsI43YIggh/KUSlhqxrMz74AkOZcWBm9QkH+Gw8NDLuITi+m0yzhluSyJzBLcpblUhFxywSRAEtPwQjsKVN30CNh0uvYuOubSHLtN3J0TO1j0pmBNuFbFWRK0gyPRZpktL2I5JQkclKakNlnh54g6ocvevUr/Ai2a7wineEkcJupA3S1Wg0nVZM2E6wbEF5U5G9Q++wI7sfR7h3N8HVNstykBfzH+KJEZwiWD0aIwxCgJ0R1Mu7GXOeJSuK93bZtQIle4D9pUNU5DC5jEsu55AvZakM5NicGyEj8uuz8bUCQgj67QfY671P3vEoeC69gy695U1U7NG7XV0pUwBjDJa1/JJlWWxzHuflzQe5FJ/GsgUVuZ2t8lEkTvfc0aNHb72flBhjVicQQqCUuvM3/M+WDVguWBrMVdDXEGZlBVEUrVCU9d9s5+fnaTQa2PZyPxEIhJaI1EEoZwX4ncynp6fXrmB4eJjR0VFarRbNZnP9P9rt9gohqNVq1Ov1ZbF/AZGev3hLJ2/zAAAAAElFTkSuQmCC)}
		.xchecked-box {list-style-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAANOgAADMQBiN+4gQAAAAd0SU1FB9gKGQ8bDYnDxEwAAAAZdEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVBXgQ4XAAAEK0lEQVRIx9WVS2hTWRjHf/eR3CY1nbxMH2YiZRQS6qO13YlMVxY3SnVcuNIBFezGpSADLoQqLu1sHJCqdCFSXFpw4YOCSH3BtFqttTNamabX3DS5bfO6uffMoglja3RGcDMHzuac7/z/53++//cd+L8P6VuCPQYZ8ADNgBd4J31DcDcQs+GnHByRocEDv0kfBSjAOlYCs11Q+gpwDegS8LMJ+3QIK0ATzEhV8Odnz5bzw8P4dJ25aJQ/WlvJ1df/K7hSLtOcTNI+Pk69rpMTgqIQhCDvh1/VSpw79+gRrRMTmLZNezLJJsPg+a5dmOEwQlFqg1sWG16/Jv7sGWXDwBACFQjC9HcwIMONKkGp4PGAJGEDS0IQmZlhnWnye3c3eiyGo6qr3WHbrJ+dJf7gAXI6zSIr72T7/fgzmT4FHnTBsgrQBfYvTU0km5vxz86iADnAm0rRPTWFt7cXZccOJJcLAGHb2K9ekT93jmwmwwdAAFpjI6Ntbfxw5879ag7l6o1sr5eHHR3IsRgeQK/M4sQE+YEB7JcvEY6zAj45SWFgAPPxYwzHoQxIkQjTPT0kIxE+Noj8sexFn4/xnh58iQTeSpHkHAdrbIzi0BCOrmNPTpK/eJHM3bt8sCyKQCiR4NWePWSiUZw1+ZLXJm4pFKLhzBlCsRh2RUXacVgeGaF47RrL58+zcP8+RrmMkCQinZ1EL1zAjERqmkH+tLYl1G3bCJw4QUjT0IA0MJfLMX/5MqmHD0nZNiUgtGULG/r7ccXjINWuWbnmqsuFu7sb/4EDNLlcBIEioNs2KUAFGmMxmk6dQm1tRZI+3xBqEkiShBQOox05Ql1nJ26gvuIUAWiKQnj/ftStW5Fk+YuF+NldsbBA4cYN9KdPmaso8Fc62ZJtk7l1C2t0FGdxESHE1xE4hkHh6lX0oSHSpRIewC/LrPf7CSgKNpCamkI/fZr8pUuIZBIcpyaBunahPp1mub+fDyMjGKUSChCsq6Nh717q9u2jbnSU0uAgRrFIwTThyhUCqRS+hgZKLS1fJvDm87SNjZGcnsYUAjcQ8vsJ9/Xh7u1FDgRQN20iks3iDA+zZFmYhQLqzZtsj8WY3L0baY2Sf55ICCKpFHUzM2SEQAJCHg+hY8fQDh5EDgRWDoRC1J88SePRo2geD0XAcBy8b98Sv3ePYDZbbf2rFQjLIphMsmDbaEBQVQkePox26BCSz7e6i4bDrDt+nGYhmBscpFAskheC4Js3bPR4qHphFYEnlcI7P4/jdqNpGu8TCe4oCsXr1z//F2ga3+/cSfTJE0qmSVYIsKzaOZDcbjKyzFIiwfvt21kMBLA07YsetzWNd+3tLLW0sH5igvT8PH9Go/z44kX+E4LGjg7GDYOcy4XlOEgLC//5P/5LCFzxOPLmzWyIx+m6fduu7v0NVGqyTSycKksAAAAASUVORK5CYII=)}
		.unchecked-box {list-style-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAANOgAADMQBiN+4gQAAAAd0SU1FB9gKGQ8qAt8h3m8AAAAZdEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVBXgQ4XAAAA60lEQVRIx+2VsQqDMBRF70sCLg5OLoKgjk7+lJ/hh+STXBwcnRz8ArMEkrxOFktbaC3tULzTg5e8k5vADXDq70VbobXmvu/hvQczg4heHrJfXxQFuq67blZbMc8zpmlCXddIkgTOuZcBUko45zCOI6y1Nz2xFSEEZFmGOI7fGg4A3nsQEZqmuXOu9jallACAtm3fvmutNaIoAjM/dkBECCF89KCbk4eAb+kEnIAT8EsAM0OIz3hSyrssUvss8t5fg+uIrLXPs0gIgWVZYIyBUurQyYdheO4gz3NUVQVjDNZ1PfSjpWmKsixvehfB9GBZ3NndrgAAAABJRU5ErkJggg==)}
		.migrated-box {list-style-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABGdBTUEAALGPC/xhBQAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAANOgAADMQBiN+4gQAAAAd0SU1FB+AKHREFA8vJSnkAAAAZdEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVBXgQ4XAAAC1klEQVRIx+2VT0hUURTGf/e958w4Tc3TYowMw1GyEgwr1MqsFmbZIrIWQZsWJUjbdoHQpl3Qps0swnCRECQFYkR/TC1iKkqmfzAKTo2Vf8hoRsfR995tkRMT87RRWkUHHhy495zvnvvu933wP/75EKkkEAjIYDCIaZpIKRFCZN0kfX9xcTGtra2/irVUEolECIfDlJeXo+s6hmFkDaCqKoZhEAqFSCaTv60pqcSyLAoLC/F4PEtqDmCaJkIIKisrMybX0sdUVRWA5ubmPzdNjjI1cpXkZC/O1fV03PXgdDqRUtpPIITAsqxsDz0Z/3CZQv8uqo4N4C8/Tp2/DdM0MiZQlvk41OTkI/LW1SGtCVb5drD3eCc71wcA+VcAHA5vDd8+3UGakxiJV7i9pdQevU5T1R35pJ3MV5QW1pf+0kWBheJ2SWua8EQXZXsu4fVVYEz1sEKvof5EuxjoPvzpcRvrdp9C2gGMx6cpOHBmbImMesds7BZubwMVDTfXDnQ3vQfK7AC8wLfExMX5whyQc3q2OEnjGm5vE76SQxsfBLr77a7CNf+n0r/l6sSMtsBSnurckiIILCobAtVZhpF4gZF4jubaymj4Ch/fd380LE7bAnjc0NPxk2yqpmEuwGxF0ag+0k5uTpzZeBeaazvj0We8fXojainsb2xh2BZgbe0gSIllzSIUh63wfQ6dZ/O2fbjcKlOj58jJ3cVENESw5yErc9nf2MLQ4jwQAkV1Lqiq37/cZ9WaahJjF9AcmxiPDhLs7ePe23oOnmUwg2hSShQle96tKDjA2HAniusgo9FxnvY9Jxw7RWzGYy92QghM00x5g53qp9sHmtB58/o2umOI2NwGolMnMYUHIb7aAyiKwsjICLqu2/qBEIKUUAohMaSHSLyBSMoPTINQ6CX5+fn2AEVFRZSUlBCPx4nFYhl3L4RESjF/GEgX3pSj+Xw+/H7/b3U/AEOZFnp7O5+5AAAAAElFTkSuQmCC)}
		ul {list-style-image: none}
		/* ul rule needed to reset style for sub-bullets */
	</style>
</head>
<body>

<!-- Header -->
<div class='header'>
	[ <a href='./PPL/VICTORIA_Marta.html'>Prev</a> ]

	[ <a href='..\..\index.html.html'>Index</a> ]

	[ <a href='./ProductivityFinland-Pohjola.html'>Next</a> ]
</div>

<hr />

<!-- Wiki content -->

<div class='pages'>
	<div class='heading'>
	<h1>Precipice-Ord <a name='V:2020:Precipice-Ord'></a></h1>
	</div>

	<div class='content'>
	<p>
Created Wednesday 10 March 2021 (21-03-10_15-20-20)<br>
<span class="zim-tag">@2021</span> <span class="zim-tag">@book</span>
</p>

<p>
Ord, Toby (2020). <i>The Precipice: Existential Risk and the Future of Humanity. </i>New York: Hachette Books.
</p>

<p>
<a href="file:///C:/Users/OWNER/Google%20Drive/Calibre_library/Toby%20Ord/The%20Precipice_%20Existential%20Risk%20and%20(1676)" title="C:\Users\OWNER\Google Drive\Calibre_library\Toby Ord\The Precipice_ Existential Risk and (1676)" class="file">C:\Users\OWNER\Google Drive\Calibre_library\Toby Ord\The Precipice_ Existential Risk and (1676)</a>
</p>

<p>
Lukulaitteesta
</p>

<p>
The Precipice
</p>

<p>
If all goes well, human history is just beginning. Humanity is about two hundred thousand years old. But the Earth will remain habitable for hundreds of millions more—enough time for millions of future generations; enough to end disease, poverty and injustice forever; enough to create heights of flourishing unimaginable today. And if we could learn to reach out further into the cosmos, we could have more time yet: trillions of years, to explore billions of worlds. Such a lifespan places present-day humanity in its earliest infancy. A vast and extraordinary adulthood awaits.
</p>

<br>

<p>
This book argues that safeguarding humanity’s future is the defining challenge of our time. For we stand at a crucial moment in the history of our species. Fueled by technological progress, our power has grown so great that for the first time in humanity’s long history, we have the capacity to destroy ourselves—severing our entire future and everything we could become.<br>
 Yet humanity’s wisdom has grown only falteringly, if at all, and lags dangerously behind. Humanity lacks the maturity, coordination and foresight necessary to avoid making mistakes from which we could never recover. As the gap between our power and our wisdom grows, our future is subject to an ever-increasing level of risk. This situation is unsustainable. So over the next few centuries, humanity will be tested: it will either act decisively to protect itself and its longterm potential, or, in all likelihood, this will be lost forever.
</p>

<br>

<p>
While this set of risks is diverse, it is also exclusive. So I will have to set aside many important risks that fall short of this bar: our topic is not new dark ages for humanity or the natural world (terrible though they would be), but the permanent destruction of humanity’s potential.
</p>

<p>
This had dramatic effects on the scale of human cooperation. Agriculture reduced the amount of land needed to support each person by a factor of a hundred, allowing large permanent settlements to develop, which began to unite together into states.12 Where the largest foraging communities involved perhaps hundreds of people, some of the first cities had tens of thousands of inhabitants. At its height, the Sumerian civilization contained around a million people.13 And 2,000 years ago, the Han dynasty of China reached sixty million people—about a hundred thousand times as many as were ever united in our forager past, and about ten times the entire global forager population at its peak.14
</p>

<p>
We cannot be sure these trends toward progress will continue. But given their tenacity, the burden would appear to be on the pessimist to explain why now is the point it will fail. This is especially true when people have been predicting such failure for so long and with such a poor track record. Thomas Macaulay made this point well:
</p>

<p>
 We cannot absolutely prove that those are in error who tell us that society has reached a turning point, that we have seen our best days. But so said all before us, and with just as much apparent reason… On what principle is it that, when we see nothing but improvement behind us, we are to expect nothing but deterioration before us?36
</p>

<p>
 And he wrote those words in 1830, before an additional 190 years of progress and failed predictions of the end of progress. During those years, lifespan doubled, literacy soared and eight in ten people escaped extreme poverty. What might the coming years bring?
</p>

<p>
How much of this future might we live to see? The fossil record provides some useful guidance. Mammalian species typically survive for around one million years before they go extinct; our close relative, Homo erectus, survived for almost two million.38 If we think of one million years in terms of a single, eighty-year life, then today humanity would be in its adolescence—sixteen years old; just coming into our power; just old enough to get ourselves in serious trouble.39<br>
 Obviously, though, humanity is not a typical species. For one thing, we have recently acquired a unique power to destroy ourselves—power that will be the focus of much of this book. But we also have unique power to protect ourselves from external destruction, and thus the potential to outlive our related species.
</p>

<p>
And a world without agony and injustice is just a lower bound on how good life could be. Neither the sciences nor the humanities have yet found any upper bound. We get some hint at what is possible during life’s best moments: glimpses of raw joy, luminous beauty, soaring love. Moments when we are truly awake. These moments, however brief, point to possible heights of flourishing far beyond the status quo, and far beyond our current comprehension.
</p>

<p>
Nuclear weapons and climate change have striking similarities and contrasts. They both threaten humanity through major shifts in the Earth’s temperature, but in opposite directions. One burst in upon the scene as the product of an unpredictable scientific breakthrough; the other is the continuation of centuries-long scaling-up of old technologies. One poses a small risk of sudden and precipitous catastrophe; the other is a gradual, continuous process, with a delayed onset—where some level of catastrophe is assured and the major uncertainty lies in just how bad it will be. One involves a classified military technology controlled by a handful of powerful actors; the other involves the aggregation of small effects from the choices of everyone in the world.
</p>

<p>
These threats to humanity, and how we address them, define our time. The advent of nuclear weapons posed a real risk of human extinction in the twentieth century. With the continued acceleration of technology, and without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues. Because these anthropogenic risks outstrip all natural risks combined, they set the clock on how long humanity has left to pull back from the brink.
</p>

<p>
What I am claiming is that there has been a robust trend toward increases in the power of humanity which has reached a point where we pose a serious risk to our own existence. How we react to this risk is up to us.<br>
 Nor am I arguing against technology. Technology has proved itself immensely valuable in improving the human condition. And technology is essential for humanity to achieve its longterm potential. Without it, we would be doomed by the accumulated risk of natural disasters such as asteroid impacts.
</p>

<p>
The problem is not so much an excess of technology as a lack of wisdom.57 Carl Sagan put this especially well:
</p>

<p>
 Many of the dangers we face indeed arise from science and technology—but, more fundamentally, because we have become powerful without becoming commensurately wise. The world-altering powers that technology has delivered into our hands now require a degree of consideration and foresight that has never before been asked of us.58
</p>

<p>
This idea has even been advocated by a sitting US president:
</p>

<p>
 the very spark that marks us as a species—our thoughts, our imagination, our language, our tool-making, our ability to set ourselves apart from nature and bend it to our will—those very things also give us the capacity for unmatched destruction… Technological progress without an equivalent progress in human institutions can doom us. The scientific revolution that led to the splitting of an atom requires a moral revolution as well.
</p>

<br>

<p>
 We need to gain this wisdom; to have this moral revolution. Because we cannot come back from extinction, we cannot wait until a threat strikes before acting—we must be proactive. And because gaining wisdom or starting a moral revolution takes time, we need to start now.
</p>

<p>
faced around a one in a hundred risk of human extinction or the unrecoverable collapse of civilization. Given everything I know, I put the existential risk this century at around one in six: Russian roulette.62 (See table 6.1 here for a breakdown of the risks.) If we do not get our act together, if we continue to let our growth in power outstrip that of wisdom, we should expect this risk to be even higher next century, and each successive century.<br>
 These are the greatest risks we have faced.63 If I’m even roughly right about their scale, then we cannot survive many centuries with risk like this. It is an unsustainable level of risk.64 Thus, one way or another, this period is unlikely to last more than a small number of centuries.65 Either humanity takes control of its destiny and reduces the risk to a sustainable level, or we destroy ourselves.
</p>

<p>
The crucial role we fill, as moral beings, is as members of a cross-generational community, a community of beings who look before and after, who interpret the past in light of the present, who see the future as growing out of the past, who see themselves as members of enduring families, nations, cultures, traditions.<br>
 —Annette Baier
</p>

<p>
Human extinction would foreclose our future. It would destroy our potential. It would eliminate all possibilities but one: a world bereft of human flourishing. Extinction would bring about this failed world and lock it in forever—there would be no coming back.<br>
 The philosopher Nick Bostrom showed that extinction is not the only way this could happen: there are other catastrophic outcomes in which we lose not just the present, but all our potential for the future.2<br>
 Consider a world in ruins: an immense catastrophe has triggered a global collapse of civilization, reducing humanity to a pre-agricultural state. During this catastrophe, the Earth’s environment was damaged so severely that it has become impossible for the survivors to ever re-establish civilization. Even if such a catastrophe did not cause our extinction, it would have a similar effect on our future. The vast realm of futures currently open to us would have collapsed to a narrow range of meager options. We would have a failed world with no way back.<br>
 Or consider a world in chains: in a future reminiscent of George Orwell’s Nineteen Eighty-Four, the entire world has become locked under the rule of an oppressive
</p>

<p>
First, I am understanding humanity’s longterm potential in terms of the set of all possible futures that remain open to us.4 This is an expansive idea of possibility, including everything that humanity could eventually achieve, even if we have yet to invent the means of achieving it.5 But it follows that while our choices can lock things in, closing off possibilities, they can’t open up new ones. So any reduction in humanity’s potential should be understood as permanent. The challenge of our time is to preserve our vast potential, and to protect it against the risk of future destruction. The ultimate purpose is to allow our descendants to fulfill our potential, realizing one of the best possible futures open to us.
</p>

<br>

<p>
Existential risks threaten the destruction of humanity’s potential. This includes cases where this destruction is complete (such as extinction) and where it is nearly complete, such as a permanent collapse of civilization in which the possibility for some very minor types of flourishing remain, or where there remains some remote chance of recovery.6 I leave the thresholds vague, but it should be understood that in any existential catastrophe the greater part of our potential is gone and very little remains.
</p>

<p>
It is not that I think only humans count. Instead, it is that humans are the only beings we know of that are responsive to moral reasons and moral argument—the beings who can examine the world and decide to do what is best. If we fail, that upward force, that capacity to push toward what is best or what is just, will vanish from the world.
</p>

<br>

<p>
This is the familiar type of probability used in courtrooms, banks and betting shops. When I speak of the probability of an existential catastrophe, I will mean the credence humanity should have that it will occur, in light of our best evidence.
</p>

<p>
There are many utterly terrible outcomes that do not count as existential catastrophes.<br>
 One way this could happen is if there were no single precipitous event, but a multitude of smaller failures. This is because I take on the usual sense of catastrophe as a single, decisive event, rather than any combination of events that is bad in sum. If we were to squander our future simply by continually treating each other badly, or by never getting around to doing anything great, this could be just as bad an outcome but wouldn’t have come about via a catastrophe.<br>
 Alternatively, there might be a single catastrophe, but one that leaves open some way for humanity to eventually recover. From our own vantage, looking out to the next few generations, this may appear equally bleak. But a thousand years hence it may be considered just one of several dark episodes in the human story. A true existential catastrophe must by its very nature be the decisive moment of human history—the point where we failed.
</p>

<p>
 am talking about a world without writing, cities, law, or any of the other trappings of civilization.<br>
 This would be a very severe disaster and extremely hard to trigger. For all the historical pressures on civilizations, never once has this happened—not even on the scale of a continent.10 The fact that Europe survived losing 25 to 50 percent of its population in the Black Death, while keeping civilization firmly intact, suggests that triggering the collapse of civilization would require more than 50 percent fatality in every region of the world.11
</p>

<p>
Most disasters short of human extinction would leave our domesticated animals and plants, as well as copious material resources in the ruins of our cities—it is much easier to re-forge iron from old railings than to smelt it from ore. Even expendable resources such as coal would be much easier to access, via abandoned reserves and mines, than they ever were in the eighteenth century.13 Moreover, evidence that civilization is possible, and the tools and knowledge to help rebuild, would be scattered across the world.
</p>

<br>

<p>
million tons of coal each year and has 1.7 billion tons left (Peabody Energy, 2018). At the time of writing, coal power plants in the US hold 100 million tons of ready-to-use coal in reserve (EIA, 2019). There are about 2 billion barrels of oil in strategic reserves (IEA, 2018, p. 19), and our global civilization contains about 2,000 kg of iron in use per person (Sverdrup &amp; Olafsdottir, 2019
</p>

<p>
The question of minimal viable population also comes up when considering multi-generational space travel. Marin &amp; Beluffi (2018) find a starting population of 98 to be adequate, whereas Smith (2014) argues for a much higher minimum of between 14,000 and 44,000. It might be possible for even smaller populations to survive, depending on the genetic technologies available to minimize risks of inbreeding and genetic drift.
</p>

<p>
If our species does destroy itself, it will be a death in the cradle—a case of infant mortality.21
</p>

<p>
 And because, in expectation, almost all of humanity’s life lies in the future, almost everything of value lies in the future as well: almost all the flourishing; almost all the beauty; our greatest achievements; our most just societies; our most profound discoveries.22 We can continue our progress on prosperity, health, justice, freedom and moral thought. We can create a world of wellbeing and flourishing that challenges our capacity to imagine. And if we protect that world from catastrophe, it could last millions of centuries. This is our potential—what we could achieve if we pass the Precipice and continue striving for a better world.
</p>

<p>
People matter equally regardless of their temporal location too. Our lives matter just as much as those lived thousands of years ago, or those a thousand years hence.24 Just as it would be wrong to think that other people matter less the further they are from you in space, so it is to think they matter less the further away from you they are
</p>

<p>
But the possibility of preventable existential risks in our lifetimes shows that there are issues where our actions can have sustained positive effects over the whole longterm future, and where we are the only generation in a position to produce those effects.26 So the view that people in the future matter just as much as us has deep practical implications. We have a long way to go if we are to understand these and integrate them fully into our moral thinking.<br>
 Considerations like these suggest an ethic we might call longtermism, which is especially concerned with the impacts of our actions upon the longterm future.
</p>

<p>
When the subtleties of the problem are taken into account and discounting is correctly applied, the future is accorded an extremely high value. The mathematical details would take us too far afield, but for now it suffices to note that discounting human wellbeing (as opposed to instrumental goods such as money), purely on the basis of distance away from us in time, is deeply implausible—especially over the long time periods we are discussing. It implies, for example, that if you can save one person from a headache in a million years’ time, or a billion people from torture in two million years, you should save the one from a headache.29
</p>

<p>
But I now see that this is no better than the old argument by the philosopher Epicurus that your death cannot be bad for you, since you are not there to experience it. What this neglects is that if I step out into the traffic and die, my life as a whole will be shorter and thereby worse: not by having more that is bad, but by containing less of everything that makes life good. That is why I shouldn’t do it. While Epicurus’s argument may provide consolation in times of grief or fear, it is not fit to be a guide for action, and no one treats it so. Imagine a government using it as the basis for our policies on safety or healthcare—or for our laws on murder.
</p>

<p>
What’s more, the future is not the only moral lens through which to view existential catastrophe. It is the one that grips me most, and that most persuades me to devote my time and energy to this issue, but there are other lenses, drawing on other moral traditions. So let us briefly explore how concern about existential risk could also spring from considerations of our past, our character and our cosmic significance. And thus how people with many different understandings of morality could all end up at this common conclusion.
</p>

<p>
making some small improvements of our own, and passing it all down to our children. Without this cooperation we would have no houses or farms, we would have no traditions of dance or song, no writing, no nations.34<br>
 This idea was beautifully expressed by the conservative political theorist Edmund Burke. In 1790 he wrote of society:
</p>

<p>
 It is a partnership in all science; a partnership in all art; a partnership in every virtue, and in all perfection. As the ends of such a partnership cannot be obtained except in many generations, it becomes a partnership not only between those who are living, but between those who are living, those who are dead, and those who are to be born.35
</p>

<p>
 This might give us reasons to safeguard humanity that are grounded in our past—obligations to our grandparents, as well as our grandchildren.<br>
 Our ancestors set in motion great projects for humanity that are too big for any single generation to achieve.
</p>

<p>
The time will come when diligent research over long periods will bring to light things which now lie hidden. A single lifetime, even though entirely devoted to the sky, would not be enough for the investigation of so vast a subject… And so this knowledge will be unfolded only through long successive ages. There will come a time when our descendants will be amazed that we did not know things that are so plain to them… Let us be satisfied with what we have found out, and let our descendants also contribute something to the truth… Many discoveries are reserved for ages still to come, when memory of us will have been effaced.36
</p>

<p>
 It is astounding to be spoken to so directly across such a gulf of time, and to see this 2,000-year plan continue to unfold.37<br>
 A human, or an entire generation, cannot complete such grand projects. But humanity can. We work together, each generation making a little progress while building up the capacities, resources and institutions to empower future generations to take the next step.
</p>

<p>
they smiled, telling me that this wasn’t how it worked—that one doesn’t repay one’s parents. One passes it on.<br>
 My parents aren’t philosophers. But their remarks suggest another way in which the past could ground our duties to the future. Because the arrow of time makes it so much easier to help people who come after you than people who come before, the best way of understanding the partnership of the generations may be asymmetrical, with duties all flowing forwards in time—paying it forwards. On this view, our duties to future generations may thus be grounded in the work our ancestors did for us when we were future generations.38
</p>

<br>

<p>
Finally, we might have duties to the future arising from the flaws of the past. For we might be able to make up for some of our past wrongs. If we failed now, we could never fulfill any duties we might have to repair the damage we have done to the Earth’s environment—cleaning up our pollution and waste; restoring the climate to its pre-industrial state; returning ecosystems to their vanished glory. Or consider that some of the greatest injustices have been inflicted not by individuals upon individuals, but by groups upon groups: systematic persecution, stolen lands, genocides. We may have duties to properly acknowledge and memorialize these wrongs; to confront the acts of our past. And there may yet be ways for the beneficiaries of these acts to partly remedy them or atone for them. Suffering an existential catastrophe would remove any last chance to do so.
</p>

<p>
In his celebrated account of virtue, Aristotle suggested that our virtues are governed and guided by a form of practical wisdom. This fits well with the idea of civilizational virtues too. For as our power continues to grow, our practical wisdom needs to grow with it.
</p>

<p>
If we are the only moral agents that will ever arise in our universe—the only beings capable of making choices on the grounds of what is right and wrong—then responsibility for the history of the universe is entirely on us. This is the only chance ever to shape the universe toward what is right, what is just, what is best for all. If we fail, then the potential not just of humanity, but of all moral action, will have been irrevocably squandered.
</p>

<p>
We shall sometimes take this a step further, exploring ethics from the perspective of humanity.47 Not just our present generation, but humanity over deep time: reflecting on what we achieved in the last 10,000 generations and what we may be able to achieve in the eons to come.<br>
 This perspective allows us to see how our own time fits into the greater story, and how much is at stake. It changes the way we see the world and our role in it, shifting our attention from things that affect the fleeting present, to those that could make fundamental alterations to the shape of the longterm future. What matters most for humanity? And what part in this plan should our generation play? What part should I play?48
</p>

<p>
Applying this perspective to humanity as a whole is increasingly useful and important. Humanity was splintered into isolated peoples for nearly the entire time since civilization began. Only recently have we found each other across the seas and started forming a single global civilization. Only recently have we discovered the length and shape of our long history, or the true potential of our future. And only recently have we faced significant threats that require global coordination.<br>
 We shouldn’t always take this perspective. Many moral challenges operate at the personal level, or the level of smaller groups. And even when it comes to the big-picture questions, it is sometimes more important to focus on the ways in which humanity is divided: on our differing power or responsibility. But just as we’ve seen the value of occasionally adopting a global perspective, so too is it important to sometimes step further back and take the perspective of humanity.
</p>

<p>
the case for making existential risk a global priority does not require certainty, for the stakes aren’t balanced. If we make serious investments to protect humanity when we had no real duty to do so, we would err, wasting resources we could have spent on other noble causes. But if we neglect our future when we had a real duty to protect it, we would do something far worse
</p>

<p>
So long as we find the case for safeguarding our future quite plausible, it would be extremely reckless to neglect it.50<br>
 Even if someone were so pessimistic about the future as to think it negative in expectation—that the heights we might reach are more than matched by the depths to which we might sink—there is still good reason to protect our potential.51 For one thing, some existential catastrophes (such as permanent global totalitarianism) would remain uncontroversially terrible and thus worthy of our attention. But there is a deeper reason too. In this case there would be immense value of information in finding out more about whether our future will be positive or negative. By far the best strategy would be to protect humanity until we have a much more informed position on this crucial question.52
</p>

<p>
Consider the possibility of engineered pandemics, which we shall soon see to be one of the largest risks facing humanity. The international body responsible for the continued prohibition of bioweapons (the Biological Weapons Convention) has an annual budget of just $1.4 million—less than the average McDonald’s restaurant.54 The entire spending on reducing existential risks from advanced artificial intelligence is in the tens of millions of dollars, compared with the billions spent on improving artificial intelligence capabilities.55 
</p>

<p>
, we can state with confidence that humanity spends more on ice cream every year than on ensuring that the technologies we develop do not destroy us.56<br>
 In scientific research, the story is similar. While substantial research is undertaken on the risk of smaller catastrophes, those that could destroy humanity’s longterm potential are neglected. Since 1991 there have been only two published climate models on the effects of a full-scale nuclear war between the United States and Russia, even while hundreds of missiles remain minutes away from a possible launch.57 There has been tremendous work on understanding climate change, but the worst-case scenarios—such as those involving more than six degrees of warming—have received comparatively little study and are mostly ignored in official reports and policy discussions.58
</p>

<br>

<p>
Economic theory tells us that existential risk will be undervalued by markets, nations and even entire generations. While markets do a great job of supplying many kinds of goods and services, there are some kinds that they 
</p>

<p>
Consider clean air. When air quality is improved, the benefit doesn’t go to a particular individual, but is shared by everyone in the community. And when I benefit from cleaner air, that doesn’t diminish the benefit you get from it. Things with these two properties are called public goods and markets have trouble supplying them.59 We typically resolve this at a local or national level by having governments fund or regulate the provision of public goods.<br>
 Protection from existential risk is a public good:
</p>

<p>
The same effect that causes this undersupply of protection causes an oversupply of risk. Since only 1 percent of the damages of existential catastrophe are borne by the people of the United Kingdom, their government is incentivized to neglect the downsides of risk-inducing policies by this same factor of 100. (The situation is even worse if individuals or small groups become able to pose existential risks.
</p>

<p>
And even if we could overcome these differences and bargain toward effective treaties and policies on existential risk, we would face a final problem. The beneficiaries are not merely global, but intergenerational—all the people who would ever live. Protection from existential risk is an intergenerational global public good. So even the entire population of the globe acting in concert could be expected to undervalue existential risks by a very large factor,
</p>

<p>
One exception to this is when there is an active constituency pushing for the early action: their goodwill acts as a kind of immediate benefit. Such constituencies are most powerful when the benefits of the policy are concentrated among a small fraction of society, as this makes it worth their while to take political action. But in the case of existential risk, the benefits of protection are diffused across all citizens, leaving no key constituency to take ownership of the issue. This is a reason for neglect, albeit one that is surmountable. When citizens are empathetic and altruistic, identifying with the plight of others—as we have seen for the environment, animal rights and the abolition of slavery—they can be enlivened with the passion and determination needed to hold their leaders to account.
</p>

<br>

<p>
Behavioral psychology has identified two more reasons why we neglect existential risk, rooted in the heuristics and biases we use as shortcuts for making decisions in a complex world.62 The first of these is the availability heuristic. This is a tendency for people to estimate the likelihood of events based on their ability to recall examples. This stirs strong feelings about avoiding repeats of recent tragedies (especially those that are vivid or widely reported). But it means we often underweight events which are rare enough that they haven’t occurred in our lifetimes, or which are without precedent. Even when experts estimate a significant probability for an unprecedented event, we have great difficulty believing it until we see 
</p>

<p>
with existential risks it fails completely. For by their very nature, we never have any experience of existential catastrophe before it is too late. If only seeing is believing, we will step blindly over the precipice.<br>
 Our need
</p>

<p>
We also suffer from a bias known as scope neglect. This is a lack of sensitivity to the scale of a benefit or harm. We have trouble caring ten times more about something when it is ten times as important. And once the stakes get to a certain point, our concern can saturate.63 For example, we tend to treat nuclear war as an utter disaster, so we fail to distinguish nuclear wars between nations with a handful of nuclear weapons (in which millions would die) from a nuclear confrontation with thousands of nuclear weapons (in which a thousand times as many people would die, and our entire future may be destroyed
</p>

<p>
I am hopeful. Because there is a final reason: existential risk is very new. So there hasn’t yet been time for us to incorporate it into our civic and moral traditions. But the signs are good that this could change.
</p>

<p>
When an isolated band or tribe died out during a time of extreme hardship, the last survivors will have sometimes wondered whether they were the last of their kind, or whether others like them lived on elsewhere. But there appears to have been very little careful thought about the possibility and importance of human extinction until very recently.64<br>
 It wasn’t until the mid-twentieth century, with the creation of nuclear weapons, that human extinction moved from a remote possibility (or a certainty remote in time) to an imminent danger. Just three days after the devastation of Hiroshima, Bertrand Russell began writing his first essay on the implications for the future of humanity.65 And not long after, many of the scientists who created these weapons formed the Bulletin of the Atomic Scientists to lead the conversation about
</p>

<p>
Modern thinking on existential risk can be traced through John Leslie, whose 1996 book The End of the World broadened the focus from nuclear war to human extinction in general. After reading Leslie’s work, Nick Bostrom took this a step further: identifying and analyzing the broader class of existential risks that are the focus of this book.
</p>

<p>
Who knows whether, when a comet shall approach this globe to destroy it, as it often has been and will be destroyed, men will not tear rocks from their foundations by means of steam, and hurl mountains, as the giants are said to have done, against the flaming mass?—and then we shall have traditions of Titans again, and of wars with Heaven.<br>
 —Lord Byron
</p>

<p>
As it became increasingly clear that the Earth was vulnerable to major asteroid and comet impacts, people began to take this threat seriously. First in works of science fiction, then science.9 Alvarez’s hypothesis that an asteroid caused the last great mass extinction inspired Shoemaker to convene a seminal meeting in 1981, founding the scientific field of impact hazards. The scientists developed an ambitious proposal for finding and tracking asteroids. And in light of the growing public interest in the impact threat, it began to acquire bipartisan support in the United States Congress.10 In 1994 Congress issued NASA a directive: find and track 90 percent of all near-Earth Objects greater than one kilometer across.11
</p>

<p>
Those above ten kilometers across (the size of the one that killed the dinosaurs) threaten mass extinction. It is possible that humans would survive the cataclysm, but there is clearly a serious risk of our extinction. Last time all land-based vertebrates weighing more than five kilograms were killed.14
</p>

<p>
While uncertainties remain, the overall story here is one of humanity having its act together. It was just 12 years from the first scientific realization of the risk of global catastrophe to the point where government started taking it seriously. And now, 28 years later, almost all the large asteroids have been tracked. There is international cooperation, with a United Nations–sanctioned organization and an international alliance of spaceguard programs.19 The work is well managed and NASA funding has increased more than tenfold between 2010 and 2016.20 In my view, no other existential risk is as well handled as that of asteroids and comets.
</p>

<p>
Unfortunately, it is not clear whether we would realistically have the capability to successfully deflect asteroids more than a few kilometers across—those that concern us most.24
</p>

<p>
There is active debate about whether more should be done to develop deflection methods ahead of time.25 A key problem is that methods for deflecting asteroids away from Earth also make it possible to deflect asteroids toward Earth. This could occur by accident (e.g., while capturing asteroids for mining), or intentionally (e.g., in a war, or in a deliberate attempt to end civilization). Such a self-inflicted asteroid impact is extremely unlikely, yet may still be the bigger risk.26 After all, the entire probability of collision from one-kilometer or greater asteroids currently stands at one in 120,000 this century—we would require extreme confidence to say that the additional risk due to human interference was smaller than that.
</p>

<p>
Experts on supervolcanic eruptions do not typically suggest that there is a direct threat of human extinction. While there was some early evidence that the Toba eruption may have nearly destroyed humanity 74,000 years ago, newer evidence has made this look increasingly unlikely.32
</p>

<p>
A supernova or gamma ray burst close to our Solar System could have catastrophic effects. While the gamma rays and cosmic rays themselves won’t reach the Earth’s surface, the reactions they trigger in our atmosphere may pose a threat. The most important is probably the production of nitrogen oxides that would alter the Earth’s climate and dramatically erode the ozone layer. This last effect is thought to be the most deadly, leaving us much more exposed to UV radiation for a period of years.44
</p>

<p>
Some threats are known to be vanishingly unlikely. For example, the passage of a star through our Solar System could disrupt planetary orbits, causing the Earth to freeze or boil or even crash into another planet. But this has only a one in 100,000 chance over the next 2 billion years.50 This could also happen due to chaotic instabilities in orbital dynamics, but again this is exceptionally unlikely. Some physical theories suggest that the vacuum of space itself may be unstable, and could “collapse” to form a true vacuum. This would spread out at the speed of light, destroying all life in its wake. However, the chance of this happening cannot be higher than one in 10 million per century and is generally thought to be much lower.51<br>
 Some threats are not existential—they offer no plausible pathway to our extinction or permanent collapse.
</p>

<p>
It is striking how recently many of these risks were discovered. Magnetic field reversal was discovered in 1906. Proof that Earth had been hit by a large asteroid or comet first emerged in 1960. And we had no idea gamma ray bursts even existed until 1969.
</p>

<p>
And there is no reason to think that the flurry of discovery has finished—that we are the first generation to have discovered all the natural risks we face. Indeed, it would surely be premature to conclude that we have discovered all of the possible mechanisms of natural extinction while major mass-extinction events remain unexplained.
</p>

<p>
Luckily, there is a way out—a way of directly estimating the total natural risk. We achieve this not by considering the details of asteroid craters or collapsing stars, but by studying the remains of the species they threatened. The fossil record is our richest source of information about how long species like us survived, and so about the total extinction risk they faced.54 We will explore three ways of using the fossil record to place upper bounds on the natural extinction risk we face, all of which yield comforting results.55 However, as this method only applies directly to extinction risk,
</p>

<p>
How high could natural extinction risk be? Imagine if it were as high as 1 percent per century. How long would humanity survive? Just 100 centuries, on average. But we know from the fossil record that Homo sapiens has actually lived for about 2,000 centuries.57 At 1 percent risk per century, it would be nearly impossible to last that long: there would be a 99.9999998 percent chance of going extinct before that. So we can safely rule out a total risk of 1 percent or greater. Just how much risk could there realistically have been? We can use the longevity of Homo sapiens to form both a best-guess estimate and an upper bound for this risk.
</p>

<p>
There is an interesting ongoing debate among statisticians about what probability to assign in such cases.59 
</p>

<p>
We can also use our survival so far to make an upper bound for the total natural extinction risk. For example, if the risk were above 0.34 percent per century there would have been a 99.9 percent chance of going extinct before now.60 We thus say that risk above 0.34 percent per century is ruled out at the 99.9 percent confidence level—a conclusion that is highly significant by the usual scientific standards (equivalent to a p-value of 0.001).61 So our 2,000 centuries of Homo sapiens suggests a “best-guess” risk estimate between 0 percent and 0.05 percent, with an upper bound of 0.34 percent.
</p>

<p>
If used with the methods above, these dates would imply lower probabilities of extinction per century.<br>
 Category: Homo sapiens<br>
 Years: 200,000<br>
 Best Guess: 0–0.05%<br>
 99.9% Confidence Bound: &lt; 0.34%<br>
 Category: Neanderthal split<br>
 Years: 500,000<br>
 Best Guess: 0–0.02%<br>
 99.9% Confidence Bound: &lt; 0.14%<br>
 Category: Homo<br>
 Years: 2,000,000– 3,000,000<br>
 Best Guess: 0–0.003%<br>
 99.9% Confidence Bound: &lt; 0.023%<br>
 TABLE 3.4 Estimates and bounds of total natural extinction risk per century based on how long humanity has survived so far, using three different conceptions of humanity.
</p>

<br>

<p>
 A simple version of this technique is to look at the most similar species. Our genus, Homo, contains four other species with reasonable estimates of longevity.63 They have survived between 200,000 and 1,700,000 years. If we bear a relevantly similar risk of extinction from natural catastrophes to any of these, we are looking at per century risk in the range of 0.006 to 0.05 percent.64<br>
 Alternatively we could cast a much wider net, achieving more statistical robustness at the expense of similarity to ourselves. The typical longevity of mammalian species has been estimated at around 1 million years, while species in the entire fossil record average 1 to 10 million years. These suggest a risk in the range of 0.001 to 0.01 percent per century—or lower if we think we are more robust than a typical species (see Table 3.5).<br>
 Note that all these estimates of species lifespans include other causes of extinction in addition to catastrophes, for example being slowly outcompeted by a new species that branches off from one’s own. So they will somewhat overestimate the risk of catastrophic extinction.
</p>

<p>
While there are also many factors that mitigate these effects (such as modern medicine, quarantine and disease surveillance), there remains a very plausible case that the pandemic risk to humans in the coming centuries is significantly larger than in early humans or other species used to construct the bounds on natural risks. For these reasons, it is best not to count pandemics as a natural risk, and we shall address them later.
</p>

<p>
The human race’s prospects of survival were considerably better when we were defenceless against tigers than they are today, when we have become defenceless against ourselves.<br>
 —Arnold Toynbee
</p>

<p>
The 2,000-century track record of human existence allows us to tightly bound the existential risk from natural events. These risks are real, though very unlikely to strike over the next hundred years.<br>
 But there is no such track record for the powerful industrial technologies that are also thought to pose existential risks. The 260 years we have survived since the Industrial Revolution, or the seventy-five years since the invention of nuclear weapons, are compatible with risks as high as 50 percent or as low as 0 percent over
</p>

<p>
what evidence do we have regarding these technological risks?<br>
 In this chapter, we’ll explore the science behind the current anthropogenic risks arising from nuclear weapons, climate change and other environmental degradation. (Risks from future technologies, including engineered pandemics, will be covered in the following chapter.) Our focus will be on the worst-case scenarios—in particular, whether there is a solid scientific case that they could cause human extinction or the unrecoverable collapse of civilization.
</p>

<p>
 Oppenheimer, who would lead the development of the bomb, was deeply concerned. While the others continued their calculations, he raced off across the country to personally inform his superior, Arthur Compton, that their project may pose a threat to humanity itself. In his memoir, Compton recalled the meeting:
</p>

<p>
 Was there really any chance that an atomic bomb would trigger the explosion of the nitrogen in the atmosphere or of the hydrogen in the ocean? This would be the ultimate catastrophe. Better to accept the slavery of the Nazis than to run a chance of drawing the final curtain on mankind!<br>
 Oppenheimer’s team must go ahead with their calculations. Unless they came up with a firm and reliable conclusion that our atomic bombs could not explode the air or the sea, these bombs must never be made.3
</p>

<br>

<p>
After the war, it would be revealed that their counterparts in Germany had also discovered this threat and the possibility had been escalated all the way up to Hitler
</p>

<p>
Oppenheimer returned to Berkeley, finding that Bethe had already discovered major weaknesses in Teller’s calculations.5 While they couldn’t prove it was safe to all the physicists’ satisfaction, they eventually decided to move on to other topics. Later, Oppenheimer commissioned a secret scientific report into the possibility of igniting the atmosphere.6 It supported Bethe’s conclusions that this didn’t seem possible, but could not prove its impossibility nor put a probability on it.7 Despite the report concluding that “the complexity of the argument and the absence of satisfactory experimental foundation makes further work on the subject highly desirable,” it was taken by the leadership at Los Alamos to be the final word on the matter.
</p>

<p>
Enrico Fermi, the Nobel prize–winning physicist who was also present at the Berkeley meeting, remained concerned that deficiencies in their approximations or assumptions might have masked a true danger. He and Teller kept rechecking the analysis, right up until the day of the test.9
</p>

<p>
Physicists with a greater understanding of nuclear fusion and with computers to aid their calculations have confirmed that it is indeed impossible.11 And yet, there had been a kind of risk. The bomb’s designers didn’t know whether or not igniting the atmosphere was physically possible, so at that stage it was still epistemically possible. While it turned out that there was no objective risk, there was a serious subjective risk that their bomb might destroy humanity.<br>
 This was a new kind of dilemma for modern science. Suddenly, we were unleashing so much energy that we were creating temperatures unprecedented in Earth’s entire history. Our destructive potential had grown so high that for the first time the question of whether we might destroy all of humanity needed to be asked—and answered. So I date the beginning of the Precipice (our age of heightened risk) to 11:29 a.m. (UTC) on July 16, 1945: the precise moment of the Trinity test.
</p>

<p>
Japan was in retreat and there was no concern about losing the war. The risk was taken for the same reasons that the bombs would be dropped in Japan a month later: to shorten the war, to avoid loss of life in an invasion, to achieve more favorable surrender terms, and to warn the Soviet Union about America’s new-found might. These are not strong reasons for unilaterally risking the future of humanity.<br>
 Just how much risk did they take? It is
</p>

<p>
Of the two major thermonuclear calculations made that summer in Berkeley, they got one right and one wrong. It would be a mistake to conclude from this that the subjective risk of igniting the atmosphere was as high as 50 percent.20 But it was certainly not a level of reliability on which to risk our future.
</p>

<p>
In theory, nuclear weapons could create enough fallout to cause a deadly level of radiation over the entire surface of the Earth. But we now know this would require ten times as many weapons as we currently possess.31 Even a deliberate attempt to destroy humanity by maximizing fallout (the hypothetical cobalt bomb) may be beyond our current abilities.32
</p>

<p>
Our current best understanding comes from the work of Alan Robock and colleagues.34 While early work on nuclear winter was limited by primitive climate models, modern computers and interest in climate change have led to much more sophisticated techniques. Robock applied an ocean-atmosphere general circulation model and found an amount of cooling similar to early estimates, lasting about five times longer. This suggested a more severe effect, since this cooling may be enough to stop almost all agriculture, and it is much harder to survive five years on stockpiled food.
</p>

<p>
Most of the harm to agriculture would come from the cold, rather than the darkness or drought. The main mechanism is to greatly reduce the length of the growing season (the number of days in a row without frost). In most places this reduced growing season would be too short for most crops to reach maturity. Robock predicts that a full-scale nuclear war would cause the Earth’s average surface temperature to drop by about 7°C for about five years (then slowly return to normal over about ten more years). For comparison, this is about as cool as the Earth’s last glacial period (an “ice age”).35 As with climate change, this global average can be deceptive since some areas would cool much more than others. Summer temperatures would drop by more than 20°C over much of North America and Asia, and would stay continually below
</p>

<p>
For all that, nuclear winter appears unlikely to lead to our extinction. No current researchers on nuclear winter are on record saying that it would and many have explicitly said that it is unlikely.38 Existential catastrophe via a global unrecoverable collapse of civilization also seems unlikely, especially if we consider somewhere like New Zealand (or the southeast of Australia) which is unlikely to be directly targeted
</p>

<p>
Skeptics of the nuclear winter scenario often point to these remaining uncertainties, as they show that our current scientific understanding is compatible with a milder nuclear winter. But uncertainty cuts both ways. The effect of nuclear winter could also be more severe than the central estimates. We don’t have a principled reason for thinking that the uncertainty here makes things better.41
</p>

<p>
Since I am inclined to believe that the central nuclear winter scenario is not an existential catastrophe, the uncertainty actually makes things worse by leaving this possibility open. If a nuclear war were to cause an existential catastrophe, this would presumably be because the nuclear winter effect was substantially worse than expected, or because of other—as yet unknown—effects produced by such an unprecedented assault on the Earth.<br>
 It would therefore be very valuable to have additional research on the uncertainties surrounding nuclear winter, to see if there is any plausible combination that could lead to a much deeper or longer winter, and to have fresh research on other avenues by which full-scale nuclear war might pose an existential risk.
</p>

<br>

<p>
What about an amplifying feedback effect that causes massive warming, but stops short of boiling the oceans? This is known as a moist greenhouse effect, and if the effect is large enough it may be just as bad as a runaway.59 This may also be impossible from anthropogenic emissions alone, but the science is less clear. A recent high-profile paper suggests it may be possible for carbon emissions to trigger such an effect (leading to 40°C of warming in their simulation).60 However, there are some extreme simplifications in their model and it remains an open question whether this is really possible on Earth.61
</p>

<p>
For example, about 55 million years ago in a climate event known as the Palaeocene-Eocene Thermal Maximum (PETM), temperatures climbed from about 9°C above pre-industrial temperatures to about 14°C over about 20,000 years. Scientists have suggested that this was caused by a major injection of carbon into the atmosphere, reaching a concentration of 1,600 ppm or more.62 This provides some evidence that such a level of emissions and warming produces neither a moist greenhouse effect nor a mass extinction.
</p>

<br>

<p>
there are substantial disanalogies between now and then, most notably that the rate of warming is substantially greater today, as is the rate of growth in emissions (and the rates of change might matter as much as the levels).
</p>

<br>

<p>
If we do not restrain emissions and eventually burn 5,000 Gt C of fossil fuels, the leading Earth system models suggest we’d suffer about 9°C to 13°C of warming by the year 2300.72
</p>

<p>
In fact, human activity has already released more than an entire biosphere worth of carbon into the atmosphere.75
</p>

<p>
We often hear numbers that suggest much more precision than this: that we are now headed for 5°C warming or that certain policies are needed if we are to stay under 4°C of warming. But these expressions simplify so much that they risk misleading. They really mean that we are headed for somewhere between 2.5°C and 7.5°C of warming or that certain policies are required in order to have a decent chance of staying under 4°C (sometimes defined as a 66 percent chance, sometimes just 50 percent).84
</p>

<p>
This doesn’t rule out unknown mechanisms. We are considering large changes to the Earth that may even be unprecedented in size or speed. It wouldn’t be astonishing if that directly led to our permanent ruin. The best argument against such unknown mechanisms is probably that the PETM did not lead to a mass extinction, despite temperatures rapidly rising about 5°C, to reach a level 14°C above pre-industrial temperatures.90 But this is tempered by the imprecision of paleoclimate data, the sparsity of the fossil record, the smaller size of mammals at the time (making them more heat-tolerant), and a reluctance to rely on a single example. Most importantly, anthropogenic warming could be over a hundred times faster than warming during the PETM, and rapid warming has been suggested as a contributing factor in the end-Permian mass extinction, in which 96 percent of species went extinct.91 In the end, we can say little more than that direct existential risk from climate change appears very small, but cannot yet be ruled out.
</p>

<p>
the improvements in agriculture are just part of the story. The entire picture of overpopulation has changed. Population growth is almost always presented as an exponential process—increasing by a fixed percentage each year—but in fact that is rarely the case. From about 1800 to 1960 the world population was growing much faster than an exponential. The annual growth rate was itself growing from 0.4 percent all the way to an unprecedented rate of 2.2 percent in 1962. These trends rightly warranted significant concern about the human and environmental consequences of this rapid population increase.<br>
 But suddenly, the situation changed. The population growth rate started to rapidly decline. So far it has halved, and it continues to fall. Population is now increasing in a roughly linear manner, with a fixed number of people being added each year instead of a fixed proportion. This change has been driven not by the feared increase in death rates, but by a dramatic change in fertility, as more and more countries have undergone the demographic transition to a small family size. In 1950, the average number of children born to each woman was 5.05. It is now just 2.47—not so far above the replacement rate of 2.1
</p>

<p>
Some people have gone so far as to suggest that the real extinction risk might now be declining population.100 Fertility rates in most countries outside Africa have fallen to below the replacement rate, and perhaps this will become a global trend. Even then, I don’t think there is any real cause for concern. If declining population began to pose a clear and present danger (something that couldn’t happen for at least two centuries), it would be a simple matter for public policy to encourage childbearing up to the replacement level. The available policy levers—free childcare, free education, free child healthcare and tax benefits for families—are relatively simple, non-coercive and popular.
</p>

<p>
While I don’t know of any resources whose scarcity could plausibly constitute an existential catastrophe, it is hard to completely rule it out. It is possible that we will find a resource that is scarce, performs an essential function for civilization, has no feasible alternative, can’t be adequately recycled, and where market forces won’t ration our consumption. While I am skeptical that any resources meet this description, I encourage efforts to thoroughly check whether this is so.
</p>

<br>

<p>
while extinction is a useful measure of biodiversity loss, it is not the whole story. It doesn’t capture population reductions or species disappearing locally or regionally. While “only” 1 percent of species have gone extinct on our watch, the toll on biodiversity within each region may be much higher, and this may be what matters most. From the perspective of existential risk, what matters most about biodiversity loss is the loss of ecosystem services. These are services—such as purifying water and air, providing energy and resources, or improving our soil—that plants and animals currently provide for us, but we may find costly or impossible to do ourselves.<br>
 A prominent example is the crop pollination performed by honeybees. This is often raised as an existential risk, citing a quotation attributed Einstein that “If the bee disappeared off the surface of the globe then man would only have four years of life left.” This has been thoroughly debunked: it is not true and Einstein didn’t say it.109 In fact, a recent review found that even if honeybees were completely lost—and all other pollinators too—this would only create a 3 to 8 percent reduction in global crop production.110
</p>

<p>
While that particular example is spurious, perhaps there are other distinct ecosystem services that are threatened and that we couldn’t live without. Or a cascading failure of ecosystem services may collectively be too much for our civilization to be able to replace. It is clear that at some level of environmental destruction this would be true, though we have little idea how close we are to such a threshold, nor whether a cascade could take us there. We need more research to find out.<br>
 As with nuclear winter and extreme global warming, we don’t know of a direct mechanism for existential risk, but are putting such pressure on the global environment that there may well be some as yet unknown consequence that would threaten our survival. We could therefore think of continuing environmental damage over the coming century as a source of unforeseen threats to humanity. These unmodeled effects may well contain most of the environmental existential risk.
</p>

<p>
The Dark Ages may return, the Stone Age may return on the gleaming wings of Science, and what might now shower immeasurable material blessings upon mankind, may even bring about its total destruction.<br>
 —Winston Churchill1
</p>

<p>
One night in 1933, the world’s pre-eminent expert on atomic science, Ernest Rutherford, declared the idea of harnessing atomic energy to be “moonshine.” And the very next morning Leo Szilard discovered the idea of the chain reaction. In 1939, Enrico Fermi told Szilard the chain reaction was but a “remote possibility,” and four years later Fermi was personally overseeing the world’s first nuclear reactor. The staggering list of eminent scientists who thought heavier-than-air flight to be impossible or else decades away is so well rehearsed as to be cliché. But fewer know that even Wilbur Wright himself predicted it was at least fifty years away—just two years before he invented it.2
</p>

<br>

<p>
Confident denouncements by eminent scientists should certainly give us reason to be skeptical of a technology, but not to bet our lives against it—their track record just isn’t good enough for that.3
</p>

<p>
There are things we can say. For example, it would be surprising if the long-run trend toward developing technologies of increasing power didn’t continue into this century. And since it was precisely our unprecedented power that gave rise to the anthropogenic risks of the twentieth century, it would be remarkable if the coming century didn’t pose similar, or greater, risk.
</p>

<p>
Instead, we shall chart the horizon in terms of plausibility and probability. Are there plausible future technologies that would bring existential risks? Are these technologies probable enough to warrant preparations in case they do arrive? To do this, we don’t need to know the future, nor even the precise probabilities of what may occur. We just need to estimate these probabilities to the right ballpark; to see the dim outlines of the threats. This will give us a rough idea of the landscape ahead and how we might prepare for it.
</p>

<p>
For these dramatic gains to health and wealth are overall gains, taking all the ill effects into account.<br>
 Or at least this is true for most risks: those that are likely enough and common enough that the law of large numbers wins out, turning the unpredictability of the small scale into a demonstrable longterm average. We know that these everyday risks have been more than outweighed. But we don’t know whether this positive balance was due to getting lucky on a few key rolls of the dice. For instance, it is conceivable that the risk of nuclear war breaking out was serious enough to outweigh all the benefits of modern technology.<br>
 It is this that should most interest us when we look to the century ahead. Not the everyday risks and downsides that technology may bring, but whether there will be a handful of cases where it puts our entire bankroll at risk, with no subsequent way for us to make up the losses.
</p>

<br>

<p>
instead, I am compelled toward a much more ambivalent view. I don’t for a moment think we should cease technological progress—indeed if some well-meaning regime locked in a permanent freeze on technology, that would probably itself be an existential catastrophe, preventing humanity from ever fulfilling its potential.
</p>

<br>

<p>
But we do need to treat technological progress with maturity.5 We should continue our technological developments to make sure we receive the fruits of technology. Yet we must do so very carefully, and if needed, use a significant fraction of the gains from technology to address the potential dangers, ensuring the balance stays positive. Looking ahead and charting the potential hazards on our horizon is a key step.
</p>

<p>
During the next hundred years a combination of invasion and disease took an immense toll—one whose scale may never be known, due to great uncertainty about the size of the pre-existing population. We can’t rule out the loss of more than 90 percent of the population of the Americas during that century, though the number could also be much lower.12 And it is very difficult to tease out how much of this should be attributed to war and occupation, rather than disease. As a rough upper bound, the Columbian exchange may have killed as many as 10 percent of the world’s people.13
</p>

<p>
In 2012 a Dutch virologist, Ron Fouchier, published details of a gain-of-function experiment on the recent H5N1 strain of bird flu.24 This strain was extremely deadly, killing an estimated 60 percent of humans it infected—far beyond even the 1918 flu.25 Yet its inability to pass from human to human had so far prevented a pandemic. Fouchier wanted to find out whether (and how) H5N1 could naturally develop this ability. He passed the disease through a series of ten ferrets, which are commonly used as a model for how influenza affects humans. By the time it passed to the final ferret, his strain of H5N1 had become directly transmissible between mammals.<br>
 The work caused fierce controversy. Much of this was focused on the information contained in his work. The US National Science Advisory Board for Biosecurity ruled that his paper had to be stripped of some of its technical details before publication, to limit the ability of bad actors to cause a pandemic. And the Dutch government claimed it broke EU law on exporting information useful for bioweapons.
</p>

<p>
Fouchier’s research provides a clear example of well-intentioned scientists enhancing the destructive capabilities of pathogens known to threaten global catastrophe. And nor is it the only case. In the very same year a similar experiment was performed in the United States.26<br>
 Of course, such experiments are done in secure labs, with stringent safety standards. It is highly unlikely that in any particular case the enhanced pathogens would escape into the wild. But just how unlikely? Unfortunately, we don’t have good data, due to a lack of transparency about incident and escape rates.27 This prevents society from making well-informed decisions balancing the risks and benefits of this research, and it limits the ability of labs to learn from each other’s incidents. We need consistent and transparent reporting of incidents, in line with the best practices from other sectors.28 And we need serious accountability for when incidents or escapes go beyond the promised rates.
</p>

<br>

<p>
This is true even at the highest biosafety level (BSL-4). In 2001, Britain was struck by a devastating outbreak of foot-and-mouth disease in livestock. Six million animals were killed in an attempt to halt its spread, and the economic damages totalled £8 billion. Then in 2007 there was another outbreak, which was traced to a lab working on the disease. Foot-and-mouth was considered a highest category pathogen and required the highest level of biosecurity. Yet the virus escaped from a badly maintained pipe, leaking into the groundwater at the facility. After an investigation, the lab’s license was renewed—only for another leak to occur two weeks later.30 In my view, this track record of escapes shows that even BSL-4 is insufficient for working on pathogens that pose a risk of global pandemics on the scale of the 1918 flu or worse—especially if that research involves gain-of-function (and the extremely dangerous H5N1 gain-of-function research wasn’t even performed at BSL-4).31 Thirteen years since the last publicly acknowledged outbreak from a BSL-4 facility is not good enough. 
</p>

<p>
But the answer may also just be that we have too little data. The patterns of disease outbreaks, war deaths and terrorist attacks all appear to follow power law distributions. Unlike the familiar “normal” distribution where sizes are clustered around a central value, power law distributions have a “heavy tail” of increasingly large events, where there can often be events at entirely different scales, with some being thousands, or millions, of times bigger than others. Deaths from war and terror appear to follow power laws with especially heavy tails, such that the majority of the deaths happen in the few biggest events. For instance, warfare deaths in the last hundred years are dominated by the two World Wars, and most US fatalities from terrorism occurred in the September 11 attacks.45 When events follow a distribution like this, the average size of events until now systematically under-represents the expected size of events to come, even if the underlying risk stays the same.46
</p>

<p>
The Human Genome Project was the largest ever scientific collaboration in biology. It took thirteen years and $500 million to produce the full DNA sequence of the human genome. Just 15 years later, a genome can be sequenced for under $1,000 or within a single hour.47 The reverse process has become much easier too: online DNA synthesis services allow anyone to upload a DNA sequence of their choice then have it constructed and shipped to their address. While still expensive, the price of synthesis has fallen by a factor of a thousand over the last two decades and continues to drop.48 The first ever uses of CRISPR and gene drives were the biotechnology achievements of the decade. But within just two years each of these technologies were used successfully by bright students participating in science competitions.49
</p>

<p>
The most famous international protection comes from the Biological Weapons Convention (BWC) of 1972. This is an important symbol of the international taboo against these weapons and it provides an ongoing international forum for discussion of the threat. But it would be a mistake to think it has successfully outlawed bioweapons.53 There are two key challenges that limit its ability to fulfill this mission.<br>
 First, it is profoundly underfunded. This global convention to protect humanity has just four employees, and a smaller budget than an average McDonald
</p>

<p>
Biotechnology companies are working to limit the dark side of the democratization of their field. For example, unrestricted DNA synthesis would help bad actors overcome a major hurdle to creating extremely deadly pathogens. It would allow them to get access to the DNA of controlled pathogens like smallpox (whose genome is readily available online) and to create DNA with modifications to make the pathogen more dangerous.61 Therefore many synthesis companies make voluntary efforts to manage this risk, screening their orders for dangerous sequences. But the screening methods are imperfect and they only cover about 80 percent of orders.62 There is significant room for improving this process and a strong case for making screening mandatory. The challenges will only increase as desktop synthesis machines become available, preventing these from being misused may require software or hardware locks to ensure the sequences get screened.
</p>

<p>
toward risky action known as the unilateralist’s curse.65 For even when the overwhelming majority of scientists think the danger outweighs the benefit, it takes just one overly optimistic estimate to lead to the information being released.66 Contrary to good scientific practice, the community’s decision is being determined by a single outlier.<br>
 And once the information has been released, it is too late for further action. Suppressing the disclosed information, or decrying those who published it, draws even more attention. Indeed, the information about what careful people are paying attention to is another form of information hazard. Al-Qaeda was inspired to pursue bioterrorism by the Western warnings about the power and ease of these weapons.67 And the Japanese bioweapons program of the Second World War (which used the bubonic plague against China) was directly inspired by an anti-bioweapons treaty: if Western powers felt the need to outlaw their use, these weapons must be potent indeed.68
</p>

<p>
The specification of which acts and states produce reward for the agent is known as its reward function. This can either be stipulated by its designers (as in the cases above) or learned by the agent. In the latter case, the agent is typically allowed to observe expert demonstrations of the task, inferring the system of rewards that best explains the expert’s behavior.
</p>

<br>

<p>
 So any near-term attempt to align an AI agent with human values would produce only a flawed copy. Important parts of what we care about would be missing from its reward function. In some circumstances this misalignment would be mostly harmless. But the more intelligent the AI systems, the more they can change the world, and the further apart things will come.
</p>

<p>
Of course, no current AI systems can do any of these things. But the question we’re exploring is whether there are plausible pathways by which a highly intelligent AGI system might seize control. And the answer appears to be “yes.” History already involves examples of individuals with human-level intelligence (Hitler, Stalin, Genghis Khan) scaling up from the power of an individual to a substantial fraction of all global power, as an instrumental goal to achieving what they want.101 And we saw humanity scaling up from a minor species with less than a million individuals to having decisive control over the future. So we should assume that this is possible for new entities whose intelligence vastly exceeds our own—especially when they have effective immortality
</p>

<p>
the people actually working in AI are not.103 If true, this would provide good reason to be skeptical of the risk. But even a cursory look at what the leading figures in AI are saying shows it is not.<br>
 For example, Stuart Russell, a professor at the University of California, Berkeley, and author of the most popular and widely respected textbook in AI, has strongly warned of the existential risk from AGI. He has gone so far as to set up the Center for Human-Compatible AI, to work on the alignment problem.104 In industry, Shane Legg (Chief Scientist at DeepMind) has warned of the existential dangers and helped to develop the field of alignment research.105 Indeed many other leading figures from the early days of AI to the present have made similar statements.106
</p>

<p>
The main points of those who downplay the risks are that (1) we likely have decades left before AI matches or exceeds human abilities, and (2) attempting to immediately regulate research in AI would be a great mistake. Yet neither of these points is actually contested by those who counsel caution: they agree that the time frame to AGI is decades, not years, and typically suggest research on alignment, not regulation.
</p>

<p>
One of the underlying drivers of the apparent disagreement is a difference in viewpoint on what it means to be appropriately conservative. This is well illustrated by a much earlier case of speculative risk, when Leo Szilard and Enrico Fermi first talked about the possibility of an atomic bomb: “Fermi thought that the conservative thing was to play down the possibility that this may happen, and I thought the conservative thing was to assume that it would happen and take all the necessary precautions.”107 In 2015 I saw this same dynamic at the seminal Puerto Rico conference on the future of AI. Everyone acknowledged that the uncertainty and disagreement about timelines to AGI required us to use “conservative assumptions” about progress—but half used the term to allow for unfortunately slow scientific progress and half used it to allow for unfortunately quick onset of the risk. I believe much of the existing tension on whether to take risks from AGI seriously comes down to these disagreements about what it means to make responsible, conservative, guesses about future progress in AI.
</p>

<p>
Two years later an expanded conference reconvened at Asilomar, a location chosen to echo the famous genetics conference of 1975, where biologists came together to pre-emptively agree principles to govern the coming possibilities of genetic engineering. At Asilomar in 2017, the AI researchers agreed on a set of Asilomar AI Principles, to guide responsible longterm development of the field. These included principles specifically aimed at existential risk:
</p>

<p>
 Capability Caution: There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities.<br>
 Importance: Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.<br>
 Risks: Risks posed by AI systems, especially catastrophic or existential risks, must be subject to planning and mitigation efforts commensurate
</p>

<p>
Perhaps the best window into what those working on AI really believe comes from the 2016 survey of leading AI researchers. As well as asking if and when AGI might be developed, it asked about the risks: 70 percent of the researchers agreed with Stuart Russell’s broad argument about why advanced AI might pose a risk;110 48 percent thought society should prioritize AI safety research more (only 12 percent thought less). And half the respondents estimated that the probability of the longterm impact of AGI being “extremely bad (e.g., human extinction)” was at least 5 percent.111 I find this last point particularly remarkable—in how many other fields would the typical leading researcher think there is a one in twenty chance the field’s ultimate goal would be extremely bad for humanity?
</p>

<p>
If researchers can see that building AI would be extremely dangerous, then why on earth would they go ahead with it? They are not simply going to build something that they know will destroy them.112<br>
 If we were all truly wise, altruistic and coordinated, then this argument would indeed work. But in the real world people tend to develop technologies as soon as the opportunity presents itself and deal with the consequences later. One reason for this comes from the variation in our beliefs: if even a small proportion of researchers don’t believe in the dangers (or welcome a world with machines in control), they will be the ones who take the final steps. This is an instance of the unilateralist’s curse (discussed here). Another reason involves incentives: even if some researchers thought the risk was as high as 10 percent, they may still want to take it if they thought they would reap most of the benefits. This may be rational in terms of their self-interest, yet terrible for the world.
</p>

<p>
Whether we survive the development of AI with our longterm potential intact may depend on whether we can learn to align and control AI systems faster than we can develop systems capable enough to pose a threat. Thankfully, researchers are already working on a variety of the key issues, including making AI more secure, more robust and more interpretable. But there are still very few people working on the core issue of aligning AI with human values. This is a young field that is going to need to progress a very long way if we are to achieve our security.
</p>

<br>

<p>
 We need to use the downtime, when things are calm, to prepare for when things get serious in the decades to come. The time we have now is valuable, and we need to make use of it.115
</p>

<p>
Losing our potential means getting locked into a bad set of futures. We can categorize existential catastrophes by looking at which aspects of our future get locked in. This could be a world without humans (extinction) or a world without civilization (unrecoverable collapse). But it could also take the form of an unrecoverable dystopia—a world with civilization intact, but locked into a terrible form, with little or no value.116
</p>

<p>
We can divide the unrecoverable dystopias we might face into three types, on the basis of whether they are desired by the people who live in them. There are possibilities where the people don’t want that world, yet the structure of society makes it almost impossible for them to coordinate to change it. There are possibilities where the people do want that world, yet they are misguided and the world falls far short of what they could have achieved. And in between there are possibilities where only a small group wants that world but enforces it against the wishes of the rest. Each of these types has different hurdles it would need to overcome in order to become truly locked in.
</p>

<br>

<p>
Note that to count as existential catastrophes, these outcomes don’t need to be impossible to break out of, nor to last millions of years. Instead, the defining feature is that entering that regime was a crucial negative turning point in the history of human potential, locking off almost all our potential for a worthy future. One way to look at this is that when they end (as they eventually must), we are much more likely than we were before to fall down to extinction or collapse than to rise up to fulfill our potential. For example, a dystopian society that lasted all the way until humanity was destroyed by external forces would be an existential catastrophe. However, if a dystopian outcome does not have this property, if it leaves open all our chances for success once it ends—it is a dark age in our story, but not a true existential catastrophe.
</p>

<p>
 is unclear whether Hitler or Stalin had the expansionist aims to control the entire world, or the technical and social means to create truly lasting regimes.119<br>
 This may change. Technological progress has offered many new tools that could be used to detect and undermine dissent, and there is every reason to believe that this will continue over the next century. Advances in AI seem especially relevant, allowing automated, detailed monitoring of everything that happens in public places—both physical and online. Such advances may make it possible to have regimes that are far more stable than those of old.
</p>

<br>

<p>
A second kind of unrecoverable dystopia is a stable civilization that is desired by few (if any) people. It is easy to see how such an outcome could be dystopian, but not immediately obvious how we could arrive at it, or lock it in, if most (or all) people do not want it.120<br>
 The answer lies in the various population-level forces that can shape global outcomes. Well-known examples include market forces creating a race to the bottom, Malthusian population dynamics pushing down the average quality of life, or evolution optimizing us toward the spreading of our genes, regardless of the effects on what we value. These are all dynamics that push humanity toward a new equilibrium, where these forces are finally in balance. But there is no guarantee this equilibrium will be good.
</p>

<br>

<p>
Because most of the costs of pollution aren’t borne by the person who causes it, we can end up in a situation where it is in the self-interest of each person to keep engaging in such activities, despite this making us all worse off. It took significant moral progress and significant political action to help us break out of this. We may end up in new traps that are even harder to coordinate our way out of. This could be at the level of individuals, or at the level of groups. We could have nations, ideological blocs, or even planets or descendent species of Homo sapiens locked in harmful competition—doing what is best for their group, but bad for groups on the whole.
</p>

<p>
The third possibility is the “desired dystopia.”121 Here it is easier to see how universal desire for an outcome might cause us to lock it in, though less clear how such an outcome could be dystopian.
</p>

<p>
The historical record is rife with examples of seriously defective ideologies and moral views that gripped large parts of the world. Moreover, even reasonable normative views often recommend that they be locked in—for otherwise a tempting rival view may take over, with (allegedly) disastrous results.122 Even though the most plausible moral views have a lot of agreement about which small changes to the world are good and which are bad, they tend to come strongly apart in their recommendations about what an optimal world would look like. This problem thus echoes that of AI alignment, where a strong push toward a mostly correct ideal could instead spell disaster.<br>
 Some plausible examples include: worlds that completely renounce further technological progress (which ensures our destruction at the hands of natural risks),123 worlds that forever fail to recognize some key form of harm or injustice (and thus perpetuate it blindly), worlds that lock in a single fundamentalist religion, and worlds where we deliberately replace ourselves with something
</p>

<p>
Of course, we can also see lock-in on smaller scales. The Corwin Amendment to the US constitution provides a disturbing example of attempted lock-in. In an effort to placate the South and avoid civil war, the proposed Thirteenth Amendment aimed to lock in the institution of slavery by making it impossible for any future amendments to the constitution to ever abolish it.125
</p>

<p>
A key problem is that the truth of an idea is only one contributor to its memetic potential—its ability to spread and to stick. But the more that rigorous and rational debate is encouraged, the more truth contributes to memetic success. So encouraging a culture of such debate may be one way we can now help avoid this fate. (For more on this, see the discussion of the Long Reflection in Chapter 7.)
</p>

<br>

<p>
through which to think about existential risk in general. We might adopt the guiding principle of minimizing lock-in. Or to avoid the double negative, of preserving our options.127 This is closely related to the idea of preserving our longterm potential—the difference being that preserving our options takes no account of whether the options are good or bad. This is not because we intrinsically care about keeping options alive even if they are bad, but because we aren’t certain they are bad, so we risk making an irreversible catastrophic mistake if we forever foreclose an option that would turn out to be best.
</p>

<p>
Such a powerful technology may pose some existential risk. Most attention has so far focused on the possibility of creating tiny self-replicating machines that could spread to create an ecological catastrophe. This may be possible, but there are mundane dangers that appear more likely, since extreme manufacturing power and precision would probably also allow the production of new weapons of mass destruction.128 Indeed the problems resemble those of advanced biotechnology: the democratization of extremely powerful technology would allow individuals or small groups access to the kinds of power (both constructive and destructive) that was previously only available to powerful nations. Solutions to managing this technology may require digital controls on what can be fabricated or state control of fabrication (the path we took with nuclear power). While this technology is more speculative than advanced biotechnology or AI, it may also come to pose a significant risk.
</p>

<p>
In some cases, scientists confidently assert that it is impossible for the experiment to cause a disaster or extinction. But even core scientific certainties have been wrong before: for example, that objects have determinate locations, that space obeys Euclid’s axioms, and that atoms can’t be subdivided, created or destroyed. If pressed, the scientists would clarify that they really mean it couldn’t happen without a major change to our scientific theories. This is sufficient certainty from the usual perspective of seeking accurate knowledge, where 99.9 percent certainty is more than enough. But that is a standard which is independent of the stakes. Here the stakes are uniquely high and we need a standard that is sensitive to this.135<br>
 The usual approach would be to compare the expected gains to the expected losses. But that is challenging
</p>

<p>
Imagine if the scientific establishment of 1930 had been asked to compile a list of the existential risks humanity would face over the following hundred years. They would have missed most of the risks covered in this book—especially the anthropogenic risks.137 Some would have been on the edge of their awareness, while others would come as complete shocks. How much risk lies beyond the limits of our own vision?
</p>

<p>
One might wonder what good can come of considering risks so far beyond our sight. While we cannot directly work on them, they may still be lowered through our broader efforts to create a world that takes its future seriously. Unforeseen risks are thus important to understanding the relative value of broad versus narrowly targeted efforts.
</p>

<p>
Nick Bostrom has recently pointed to an important class of unforeseen risk.138 Every year as we invent new technologies, we may have a chance of stumbling across something that offers the destructive power of the atomic bomb or a deadly pandemic, but which turns out to be easy to produce from everyday materials. Discovering even one such technology might be enough to make the continued existence of human civilization impossible.
</p>

<p>
In order for our time to be uniquely safe, we must have lowered natural risk by more than we have raised anthropogenic risk. But as we saw in Chapter 3, despite the sheer number of natural threats, their combined probability must have always been extremely low (or species like ours couldn’t last as long as they do). The realistic estimates for the natural existential risk per century ranged from one in 1,000,000 to one in 2,000. So there just isn’t much risk there for our technologies to reduce. Even on the most generous of these estimates, technology could reduce natural risk by at most a twentieth of a percentage point. And we would have to be extremely optimistic about our future to think we face less anthropogenic risk than that. Would we expect to get through 2,000 centuries like this one? Should we really be 99.95 percent certain we’ll make it through the next hundred years?
</p>

<br>

<p>
Instead, I think the right method is to start with a probability that reflects our overall impressions, then adjust this in light of the scientific evidence.7 When there is a lot of evidence, these approaches converge. But when there isn’t, the starting point can matter.<br>
 In the case of artificial intelligence, everyone agrees the evidence and arguments are far from watertight, but the question is where does this leave us? Very roughly, my approach is to start with the overall view of the expert community that there is something like a one in two chance that AI agents capable of outperforming humans in almost every task will be developed in the coming century. And conditional on that happening, we shouldn’t be shocked if these agents that outperform us across the board were to inherit our future. Especially if when looking into the details, we see great challenges in aligning these agents with our values.
</p>

<br>

<p>
there are other valuable ways to classify risks too. For example, Shahar Avin and colleagues at the Cambridge Centre for the Study of Existential Risk (CSER) have classified risks according to which critical system they threaten: whether that be an essential system in the environment, in the human body or in our social structures.12
</p>

<p>
While you should feel free to disagree with my particular estimates, I think a safe case can be made that the contribution of great-power war to existential risk is larger than the contribution of all natural risks combined. So a young person choosing their career, a philanthropist choosing their cause or a government looking to make a safer world may do better to focus on great-power war than on detecting asteroids or comets.<br>
 This alternative way of carving up the 
</p>

<p>
They dubbed smoking a “risk factor,” stating: “A risk factor is an attribute or exposure which is causally associated with an increased probability of a disease or injury.” This allows an extremely useful cross-cutting analysis of where the ill health is coming from, letting us estimate how much could be gained if we were considering making inroads against risk factors such as smoking, lack of access to safe drinking water or vitamin deficiency.<br>
 Let us call something that increases existential risk an existential risk factor (the “existential” can be omitted for brevity since this is the only kind of risk factor we are concerned with hereafter).20 Where the division into individual risks can be seen as breaking existential risk up into vertical silos, existential risk factors cut across these divisions. The idea of existential risk factors is very general 
</p>

<p>
Chapter 4 focused on nuclear war and climate change in their role as existential risks (since it is important to understand whether there really are plausible mechanisms through which our potential could be destroyed). But their role as risk factors may be more important. A better understanding of how they increase other risks would be of great value, because what we ultimately want to know is how much they increase risk overall.26
</p>

<p>
factors that reduce risk.27 We can call these existential security factors.28 Examples include strong institutions for avoiding existential risk, improvements in civilizational virtues or peace between great powers. As this last example suggests, if something is a risk factor, its opposite will be a security factor.<br>
 Many of the things we commonly think of as social goods may turn out to also be existential security factors. Things such as education, peace or prosperity may help protect us. And many social ills may be existential risk factors. In other words, there may be explanations grounded in existential risk for pursuing familiar, common-sense agendas.
</p>

<p>
My colleague at the Future of Humanity Institute, Owen Cotton-Barratt, has shown that when these terms are appropriately defined, the cost-effectiveness of working on a particular problem can be expressed by a very simple formula:32
</p>

<br>
<br>
<br>
<br>

<p>
 Even though it is very difficult to assign precise numbers to any of these dimensions, this model still provides useful guidance. For example, it shows why the ideal portfolio typically involves investing resources fighting several risks instead of just one: as we invest more in a given risk, it becomes less neglected, so the priority of investing additional resources in it falls. After a while, marginal resources would be better spent on a different risk.
</p>

<br>

<p>
The philosopher Nick Beckstead suggests we distinguish between targeted and broad interventions.37 A focus on safeguarding humanity needn’t imply a focus on narrowly targeted interventions, such as the governance of a dangerous technology. Existential risk can also be reduced by broader interventions aimed at generally improving wisdom, decision-making or international cooperation. And it is an open question which of these approaches is more effective. Beckstead suggests that longtermists in past centuries would have done better to focus on broad interventions rather than narrow interventions.
</p>

<br>

<p>
In short, early action is higher leverage, but more easily wasted. It has more power, but less accuracy. If we do act far in advance of a threat, we should do so in ways that take advantage of this leverage, while being robust to near-sightedness.38 This often means a focus on knowledge and capacity building, over direct work.
</p>

<p>
There are no catastrophes that loom before us which cannot be avoided; there is nothing that threatens us with imminent destruction in such a fashion that we are helpless to do something about it. If we behave rationally and humanely; if we concentrate coolly on the problems that face all of humanity, rather than emotionally on such nineteenth century matters as national security and local pride; if we recognize that it is not one’s neighbors who are the enemy, but misery, ignorance, and the cold indifference of natural law—then we can solve all the problems that face us. We can deliberately choose to have no catastrophes at all.<br>
 —Isaac Asimov1
</p>

<p>
When exploring these issues, I find it useful to consider our predicament from humanity’s point of view: casting humanity as a coherent agent, and considering the strategic choices it would make were it sufficiently rational and wise. Or in other words, what all humans would do if we were sufficiently coordinated and had humanity’s longterm interests at heart.
</p>

<p>
GRAND STRATEGY FOR HUMANITY<br>
 How can humanity have the greatest chance of achieving its potential? I think that at the highest level we should adopt a strategy proceeding in three phases:2
</p>

<p>
 1. Reaching Existential Security<br>
 2. The Long Reflection<br>
 3. Achieving Our Potential
</p>

<p>
This will involve major changes to our norms and institutions (giving humanity the prudence and patience we need), as well as ways of increasing our general resilience to catastrophe. This needn’t require foreseeing all future risks right now. It is enough if we can set humanity firmly on a course where we will be taking the new risks seriously: managing them successfully right from their onset or sidestepping them entirely.<br>
 Note that existential security doesn
</p>

<p>
A key insight motivating existential security is that there appear to be no major obstacles to humanity lasting an extremely long time, if only that were a key global priority. As we saw in Chapter 3, we have ample time to protect ourselves against natural risks: even if it took us millennia to resolve the threats from asteroids, supervolcanism and supernovae, we would incur less than one percentage point of total risk.
</p>

<p>
We would adopt a more mature attitude to the most radical new technologies—devoting at least as much of humanity’s brilliance to forethought and governance as to technological development.
</p>

<p>
If we achieve existential security, we will have room to breathe. With humanity’s longterm potential secured, we will be past the Precipice, free to contemplate the range of futures that lie open before us. And we will be able to take our time to reflect upon what we truly desire; upon which of these visions for humanity would be the best realization of our potential. We shall call this the Long Reflection.
</p>

<p>
During the Long Reflection, we would need to develop mature theories that allow us to compare the grand accomplishments our descendants might achieve with eons and galaxies as their canvas.
</p>

<br>

<p>
The process may take place largely within intellectual circles, or within the wider public sphere. Either way, we would need to take the greatest care to avoid it being shaped by the bias or prejudice of those involved. As Jonathan Schell said regarding a similar venture, “even if every person in the world were to enlist, the endeavour would include only an infinitesimal fraction of the people of the dead and the unborn generations, and so it would need to act with the circumspection and modesty of a small minority.”
</p>

<p>
Would settling other planets bring us existential security?<br>
 The idea is based on an important statistical truth. If there were a growing number of locations which all need to be destroyed for humanity to fail, and if the chance of each suffering a catastrophe is independent of whether the others do too, then there is a good chance humanity could survive indefinitely.17<br>
 But unfortunately, this argument only applies to risks that are statistically independent. Many risks, such as disease, war, tyranny and permanently locking in bad values are correlated across different planets: if they affect one, they are somewhat more likely to affect the others too. A few risks, such as unaligned AGI and vacuum collapse, are almost completely correlated: if they affect one planet, they will likely affect all.
</p>

<p>
Space settlement is thus helpful for achieving existential security (by eliminating the uncorrelated risks) but it is by no means sufficient.19 Becoming a multi-planetary species is an inspirational project—and may be a necessary step in achieving humanity’s potential. But we still need to address the problem of existential risk head-on, by choosing to make safeguarding our longterm potential one of our central priorities.
</p>

<p>
First, we can’t rely on our current intuitions and institutions that have evolved to deal with small-or medium-scale risks.21 Our intuitive sense of fear is neither evolutionarily nor culturally adapted to deal with risks that threaten so much more than an individual life—risks of catastrophes that cannot be allowed to happen even once over thousands of years in a world containing billions of people. The same is true for our intuitive sense of the likelihood of very rare events and of when such a risk is too high. Evolution and cultural adaptation have led to fairly well-tuned judgments for these questions in our day-to-day lives (when it’s safe to cross the road; whether to buy a smoke alarm), but are barely able to cope with risks that threaten hundreds of people, let alone those that threaten billions and the very future of humanity.<br>
 The same is true of our institutions. Our systems of laws, norms and organizations for handling risk have been tuned to the small-and medium-scale risks we have faced over past centuries. They are ill-equipped to address risks so extensive that they will devastate countries across the globe; so severe that there will be no legal institutions remaining to exact punishment.<br>
 The second challenge is that we cannot afford to fail even once. 
</p>

<p>
On the one hand, they will need to be able to take strong actions, even when the evidence is short of the highest scientific standards. And yet, this puts us at risk of chasing phantoms—being asked (or forced) to make substantial sacrifices on the basis of little evidence. This poses an even greater problem if the risk involves classified elements or information hazards that cannot be opened up to public scrutiny and response. The challenges here are similar to those arising from the ability of governments to declare a state of emergency: contingency powers are essential for managing real emergencies, yet open to serious abuse.
</p>

<p>
we have to make decisions of grave importance without access to robust probabilities for the risks involved.24 This raises substantial difficulties in how we are to form probability estimates for use in decision-making surrounding existential risk.25 This problem already exists in climate change research and causes great difficulties in setting policy—especially if politicization leads to explicit or implicit biases in how people interpret the ambiguous evidence.<br>
 During the Cold War concern about existential risk from nuclear war was often disparaged on the grounds that we haven’t proved the risk is substantial. But when it comes to existential risk that would be an impossible standard. Our norms of scientific proof require experiments to be repeated many times, and were established under the assumptions that such experiments are possible and not too costly. But here neither assumption is true. As Carl Sagan memorably put it: “Theories that involve the end of the world are not amenable to experimental verification—or at least, not more than once.”26
</p>

<p>
Some of this use of near misses is systematized in the field of risk analysis. They have techniques for estimating the probability of unprecedented catastrophes based on the combination of precedented faults that would need to occur to allow it. For example, fault tree analysis was developed for evaluating the reliability of the launch systems for nuclear missiles, and is used routinely to help avoid low-frequency risks, such as plane crashes and nuclear meltdowns.27
</p>

<p>
. For example, we may not be able to directly apply the observed track record of asteroid collisions or full-scale nuclear war. From what we know, it doesn’t look like these selection effects have distorted the historical record much, but there are only a handful of papers on the topic and some of the methodological issues have yet to be resolved.28<br>
 A final challenge concerns all low-probability high-stakes risks. Suppose scientists estimate that an unprecedented technological risk has an extremely small chance of causing an existential catastrophe—say one in a trillion. Can we directly use this number in our analysis? Unfortunately not. The problem is that the chance the scientists have incorrectly estimated this probability is many times greater than one in a trillion. Recall their failure to estimate the size of the massive Castle Bravo nuclear explosion—if the chance of miscalculation were really so low there should be no such examples. So if a disaster does occur, it is much more likely to be because there was an estimation mistake and the real risk was higher, rather than because a one in a trillion event occurred.
</p>

<p>
No doubt many people would think a large shift in international governance is unnecessary or unrealistic. But consider the creation of the United Nations. This was part of a massive reordering of the international order in response to the tragedy of the Second World War. The destruction of humanity’s entire potential would be so much worse than the Second World War that a reordering of international institutions of a similar scale may be entirely justified. And while there might not be much appetite now, there may be in the near future if a risk increases to the point where it looms very large in the public consciousness, or if there is a global catastrophe that acts as a warning shot. So we should be open to blue-sky thinking about ideal international institutions, while at the same time considering smaller changes to the existing set.
</p>

<p>
As Ulrich Beck put it: “One can make two diametrically opposed kinds of assertion: global risks inspire paralysing terror, or: global risks create new room for action.”
</p>

<p>
Instead, my guess is that existential security could be better achieved with the bare minimum of internationally binding constraints needed to prevent actors in one or two countries from jeopardizing humanity’s entire future. Perhaps this could be done through establishing a kind of constitution for humanity, and writing into it the paramount need to safeguard our future,
</p>

<p>
We could also strengthen the World Health Organization’s ability to respond to emerging pandemics through rapid disease surveillance, diagnosis and control. This involves increasing its funding and powers, as well as R&amp;D on the requisite technologies. And we need to ensure that all DNA synthesis is screened for dangerous pathogens. There has been good progress toward this from synthesis companies, with 80 percent of orders currently being screened.39 But 80 percent is not enough. If we cannot reach full coverage through voluntary efforts, some form of international regulation will be needed.<br>
 Some of the most
</p>

<p>
Another promising avenue for incremental change is to explicitly prohibit and punish the deliberate or reckless imposition of unnecessary extinction risk.41 International law is the natural place for this, as those who impose such risk may well be national governments or heads of state, who could be effectively immune to mere national law.<br>
 The idea that it may be a serious crime to impose risks to all living humans and to our entire future is a natural fit with the common-sense ideas behind the law of human rights and crimes against humanity. There would be substantial practical challenges in reconciling this idea with the actual bodies of law, and in defining the thresholds required for prosecution.42 But these challenges are worth undertaking—our descendants would be shocked to learn that it used to be perfectly legal to threaten the continued existence of humanity.43
</p>

<p>
Its preamble showed a recognition that humanity’s continued existence may be at stake and that acting on this falls within the mission of the UN:
</p>

<p>
 Conscious that, at this point in history, the very existence of humankind and its environment are threatened, Stressing that full respect for human rights and ideals of democracy constitute an essential basis for the protection of the needs and interests of future generations… Bearing in mind that the fate of future generations depends to a great extent on decisions and actions taken today… Convinced that there is a moral obligation to formulate behavioural guidelines for the present generations within a broad, future-oriented perspective
</p>

<p>
Article 3: “The present generations should strive to ensure the maintenance and perpetuation of humankind.
</p>

<p>
This was noted by one of the earliest thinkers on existential risk, the philosopher J. J. C. Smart:
</p>

<p>
 Indeed what does it matter, from the perspective of possible millions of years of future evolution, that the final catastrophe should merely be postponed for (say) a couple of hundred years? Postponing is only of great value if it is used as a breathing space in which ways are found to avert the final disaster.49
</p>

<p>
’ve argued that our current predicament stems from the rapid growth of humanity’s power outstripping the slow and unsteady growth of our wisdom. If this is right, then slowing technological progress should help to give us some breathing space, allowing our wisdom more of a chance to catch up.50 Where slowing down all aspects of our progress may merely delay catastrophe, slowing down the growth of our power relative to our wisdom should fundamentally help.<br>
 I think that a more patient and prudent humanity would indeed try to limit this divergence. Most importantly, it would try to increase its wisdom.
</p>

<p>
We’ve seen how humanity is akin to an adolescent, with rapidly developing physical abilities, lagging wisdom and self-control, little thought for its longterm future and an unhealthy appetite for risk. When it comes to our own children, we design our societies to deliberately stage their access to risky technologies: for example, preventing them from driving a car until they reach an appropriate age and pass a qualifying test.<br>
 One could imagine applying a similar approach to humanity. Not relinquishing areas of technology, but accepting that in some cases we aren’t ready for them until we meet a given standard. For example, no nuclear technologies until we’ve had a hundred years without a major war. Unfortunately, there is a major challenge. Unlike the case with our own children, there are no wise adults to decide these rules. Humanity would have to lay down the rules to govern itself. And those who lack wisdom usually lack the ability to see this; those who lack patience are unlikely to delay gratification until they acquire it.<br>
 So while I think a more mature world would indeed restrain its growth in destructive capability to a level where it was adequately managed, I don’t see much value in advocating for this at the moment.
</p>

<p>
Since the world is so far from reaching such agreements, it would be ineffective (and likely counter-productive) for the few people who care about existential risk to use their energies to push for slowing down.<br>
 We should instead devote our energies to promoting the responsible deployment and governance of new technologies. We should make the case that the unprecedented power from technological progress requires unprecedented responsibility: both for the practitioners and for those overseeing them.<br>
 The great improvements in our quality of life from technology don’t come for free. They come with a shadow cost in risk.51 We focus on the visible benefits, but are accumulating a hidden debt that may one day come due.52 If we aren’t changing the pace of technology, the least we could do is to make sure we use some of the prosperity it grants us to service these debts. For example, to put even 1 percent of the benefits technology brings us back into ensuring humanity’s potential isn’t destroyed through further technological progress.
</p>

<br>

<p>
An interesting, and neglected, area of technology governance is differential technological development.57
</p>

<p>
While it may be too difficult to prevent the development of a risky technology, we may be able to reduce existential risk by speeding up the development of protective technologies relative to dangerous ones. This could be a role for research funders, who could enshrine it as a principle for use in designing funding calls and allocating grants, giving additional weight to protective technologies. And it could also be used by researchers when deciding which of several promising programs of research to pursue.
</p>

<p>
STATE RISKS &amp; TRANSITION RISKS<br>
 If humanity is under threat from substantial risk each century, we are in an unsustainable position. Shouldn’t we attempt to rush through this risky period as quickly as we can? The answer depends upon the type of risk we face.<br>
 Some risks are associated with being in a vulnerable state of affairs. Let’s call these state risks.58 
</p>

<p>
not all risks are like this.60 Some are transition risks: risks that arise during a transition to a new technological or social regime. For example the risks as we develop and deploy transformative AGI are like this, as are the risks of climate change as we transition to being a high-energy civilization. Rushing the transition may do nothing to lower these risks—indeed it could easily heighten them. But if the transition is necessary or highly desirable, we may have to go through it at some point, so mere delay is not a solution, and may also make things worse. The general prescription for these risks is neither haste nor slowness, but care and foresight.
</p>

<p>
if my analysis is correct, there is substantially more transition risk than state risk (in large part because there is more anthropogenic risk). This suggests that rushing our overall technological progress is not warranted.
</p>

<p>
we also need research on more abstract matters. We need to better understand longtermism, humanity’s potential and existential risk: to refine the ideas, developing the strongest versions of each; to understand what ethical foundations they depend upon, and what ethical commitments they imply; and to better understand the major strategic questions facing humanity.
</p>

<p>
For example, the Open Philanthropy Project has funded some of the most recent nuclear winter modeling as well as work on technical AI safety, pandemic preparedness and climate change—with a focus on the worst-case scenarios.66 At the time of writing they are eager to fund much more of this research, and are limited not by money, but by a need for great researchers to work on these problems.67<br>
 And there are already a handful of academic institutes dedicated to research on existential risk. For example, Cambridge University’s Centre for the Study of Existential Risk (CSER) and Oxford’s Future of Humanity Institute (FHI), where I work.68 Such institutes allow like-minded researchers from across the academic disciplines to come together and work on the science, ethics and policy of safeguarding humanity.
</p>

<p>
It is possible to believe that all the past is but the beginning of a beginning, and that all that is and has been is but the twilight of the dawn. It is possible to believe that all that the human mind has ever accomplished is but the dream before the awakening.<br>
 —H. G. Wells
</p>

<p>
carbon we have released, leaving the climate mostly restored and rebalanced.4 So long as we can learn to care rightly for our home, these blots on our record could be wiped clean, all within the lifespan of a typical species, and we could look forward to living out most of our days in a world free from the scars of immature times.<br>
 About ten million years hence, even the damage we have inflicted upon biodiversity is expected to have healed. This is how long it took for species diversity to fully recover from previous mass extinctions and our best guess for how long it will take to recover from our current actions.5
</p>

<p>
Many species are not wholly extinguished, but are succeeded by their siblings or their children on the evolutionary family tree. Our story may be similar. When considering our legacy—what we shall bequeath to the future—the end of our species may not be the end of us, our projects or our ultimate aspirations. Rather, we may simply be passing the baton.
</p>

<p>
We know of species around us that have survived almost unchanged for hundreds of millions of years. In 1839, a Swiss biologist first described and named the coelacanth, an ancient order of fish that arose 400 million years ago, before vanishing from the fossil record with the dinosaurs, 65 million years ago. The coelacanth was assumed to be long extinct, but ninety-nine years later, a fisherman off the coast of South Africa caught one in his net. Coelacanths were still living in Earth’s oceans, almost unchanged over all this time. They are the oldest known vertebrate species alive today, lasting more than two-thirds of the time since the vertebrates first arose.6<br>
 And there are older species still. The horseshoe crab has been scuttling through our oceans even longer, with an unbroken lineage of 450 million years. The nautilus has been here for 500 million years; sponges, for about 580 million. And these are merely lower bounds on their lifespans; who knows how much longer these hardy species will last? The oldest known species on Earth are cyanobacteria (blue-green algae), which have been with us for at least two billion years—much longer than there has been complex life, and more than half the time there has been life on Earth at all.7<br>
 What might humanity witness, if we (or our successors) were to last as long as the humble horseshoe crab?
</p>

<p>
One often hears estimates that the Earth will remain habitable for 1 or 2 billion years. These are predictions of when the oceans will evaporate due to a runaway or moist greenhouse effect, triggered by the increasing brightness of the Sun. But the Earth may become uninhabitable for complex life before that point: either at some earlier level of warming or through another mechanism. For example, scientists expect the brightening of the Sun to also slow the Earth’s plate tectonics, dampening volcanic activity. Life as we know it requires such activity; volcanoes lift essential carbon dioxide into our atmosphere. We currently have too much carbon dioxide, but we need small amounts for plants to photosynthesize. Without the carbon dioxide from volcanoes, scientists estimate that in about 800 million years photosynthesis will become impossible in 97 percent of plants, causing an extreme mass extinction. Then 500 million years later, there would be so little carbon dioxide that the remaining plants would also die—and with them, any remaining multicellular life.10
</p>

<p>
Even if humanity is very small in your picture of the world, if most of the intrinsic value of the world lies in the rest of our ecosystem, humanity’s instrumental value may yet be profound. For if we can last long enough, we will have a chance to literally save our world.<br>
 By adding
</p>

<p>
By traveling to other stars, we might save not only ourselves, but much of our biosphere. We could bring with us a cache of seeds and cells with which to preserve Earth’s species, and make green the barren places of the galaxy. If so, the good we could do for Earth-based life would be even more profound. Without our intervention, our biosphere is approaching middle age. Simple life looks to have about as much time remaining as it has had so far; complex life a little more. After that, to the best of our knowledge, life in the universe may vanish entirely. But if humanity survives, then even at that distant time, life may still be in its infancy. When I contemplate the expected timespan that Earth-based life may survive and flourish, the greatest contribution comes from the possibility of humanity
</p>

<p>
Our galaxy will remain habitable for an almost unfathomable time. Some of our nearby stars will burn vastly longer than the Sun, and each year ten new stars are born. Some individual stars last trillions of years—thousands of times longer than our Sun. And there will be millions of generations of such stars to follow.13 This is deep time. If we survive on such a cosmological scale, the present era will seem astonishingly close to the very start of the universe. But we know of nothing that makes such a lifespan impossible, or even unrealistic. We need only get our house in order.
</p>

<p>
The pressures toward prudence and wisdom are larger, in relative terms, and counsel care over haste. If rushing to acquire these technologies a century earlier diminished our chance of survival by even one part in 50 million, it would be counter-productive. And for many years hence, the value of another century’s reflection on what to do with the future—an irrevocable choice, and quite possibly the most important one humanity will ever make—will outweigh the value of extending our reach by just one part in fifty million. So our best grand strategy is one of careful reflection and prudence, with our degree of caution tuned to the slow clock of cosmological expansion.38
</p>

<p>
One person who seriously considered the stars was Frank Ramsey, the brilliant economist and philosopher, whose career was cut short when he died at just 26 years of age, in 1930. His attitude was one of heroic defiance:
</p>

<p>
 I don’t feel the least humble before the vastness of the heavens. The stars may be large, but they cannot think or love; and these are qualities which impress me far more than size does. I take no credit for weighing nearly seventeen stone. My picture of the world is drawn in perspective, and not to scale. The foreground is occupied by human beings and the stars are all small as threepenny bits.39
</p>

<p>
There is truth to this. What makes each of us special, so worthy of protection and celebration is something subtle about us, in the way that the matter of which we are comprised has been so delicately arranged as to allow us to think and love and create and dream.
</p>

<p>
And peak experiences are not merely potential dwellings—they are also pointers to possible experiences and modes of thought beyond our present understanding. Consider, for example, how little we know of how ultraviolet light looks to a finch; of how echolocation feels to a bat, or a dolphin; of the way that a red fox, or a homing pigeon, experiences the Earth’s magnetic field. Such uncharted experiences exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater? Mice know very little of music, art or humor. Toward what experiences are we as mice? What beauties are we blind to?40
</p>

<p>
In this respect, the possible quality of our future resembles its possible duration and scope. We saw how human civilization has probed only a tiny fraction of what is possible in time or space. Along each of these dimensions, we can zoom out from our present position with a dizzying expansion of scale, leading to scarcely imaginable vistas waiting to be explored. Such scales are a familiar feature of contemporary science. Our children learn early that everyday experience has only acquainted us with a tiny fraction of the physical universe.
</p>

<p>
how strange it would be if this single species of ape, equipped by evolution with this limited set of sensory and cognitive capacities, after only a few thousand years of civilization, ended up anywhere near the maximum possible quality of life.
</p>


	</div>

	<br />

	<div class='page-footer'>

	</div>

	

</div>

</body>
</html>
